{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### processes to semantic segmentation binary masks, removes tooth quadrant and type, removes overap with alveolar bone and teeth, sorts into hierarchy where each parent class does not have its own class, but is the sum of its child classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   7%|â–‹         | 14/197 [02:26<31:52, 10.45s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 120\u001b[0m\n\u001b[0;32m    117\u001b[0m img_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDatasets\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mHierarchical Datasets\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mTL_pano\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mannotated\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    118\u001b[0m output_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDatasets\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mHierarchical Datasets\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mTL_pano\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mprocessed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 120\u001b[0m \u001b[43mprocess_json_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_map_final\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 99\u001b[0m, in \u001b[0;36mprocess_json_files\u001b[1;34m(input_dir, img_dir, output_dir, class_map, class_map_final)\u001b[0m\n\u001b[0;32m     97\u001b[0m class_id \u001b[38;5;241m=\u001b[39m [i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m i \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m class_id]\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# class_id = [i - 1 if isinstance(i, int) and i > 4 else i for i in class_id]\u001b[39;00m\n\u001b[1;32m---> 99\u001b[0m combined_mask \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_binary_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolygons\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# combined_mask = (binary_mask * 255).astype(np.uint8)  # Ensure binary mask is 0 or 255\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# show all unique values in mask\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# print(np.unique(combined_mask))\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# show mask\u001b[39;00m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# Image.fromarray(combined_mask).show()\u001b[39;00m\n\u001b[0;32m    108\u001b[0m output_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(file)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 36\u001b[0m, in \u001b[0;36mcreate_binary_mask\u001b[1;34m(polygons, image_size, class_id, clss_map)\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;66;03m# show mask\u001b[39;00m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m# Image.fromarray(np.array(img)*255).show()\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m# combines img and mask but if a pixel value is less than the new value, exept for 0, keep the old value\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere((np\u001b[38;5;241m.\u001b[39marray(img)\u001b[38;5;241m*\u001b[39m \u001b[38;5;28mint\u001b[39m(cls_id \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m255\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m&\u001b[39m (mask \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m), np\u001b[38;5;241m.\u001b[39marray(img) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mint\u001b[39m(cls_id \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m255\u001b[39m), mask)\n\u001b[1;32m---> 36\u001b[0m     mask \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcls_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcls_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcls_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;66;03m# mask = np.minimum(mask, np.array(img) * int(cls_id * 255))\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# combines img and mask\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# mask += (np.array(img)*int(cls_id*255))\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;66;03m# mask = np.maximum(mask, np.array(img))  # Combine masks\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: Empty polygon in class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcls_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\_core\\multiarray.py:383\u001b[0m, in \u001b[0;36mwhere\u001b[1;34m(condition, x, y)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;124;03m    inner(a, b, /)\u001b[39;00m\n\u001b[0;32m    293\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    378\u001b[0m \n\u001b[0;32m    379\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, b)\n\u001b[1;32m--> 383\u001b[0m \u001b[38;5;129m@array_function_from_c_func_and_dispatcher\u001b[39m(_multiarray_umath\u001b[38;5;241m.\u001b[39mwhere)\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwhere\u001b[39m(condition, x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    385\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;124;03m    where(condition, [x, y], /)\u001b[39;00m\n\u001b[0;32m    387\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;124;03m           [ 0,  3, -1]])\u001b[39;00m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (condition, x, y)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from shapely.geometry import Polygon, Point\n",
    "from shapely.geometry import MultiPolygon\n",
    "from PIL import ImageDraw\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# traverses through all poolygons and class_id to create a binary mask for each class \n",
    "def create_binary_mask(polygons, image_size, class_id, clss_map):\n",
    "    # Create an empty mask with the size of the image\n",
    "    mask = np.zeros((image_size[1], image_size[0]), dtype=np.uint8)\n",
    "\n",
    "    # adds 1 to class_id to avoid 0 class\n",
    "    class_id = [i+1 if i != None else i for i in class_id]\n",
    "    # devides class_id by 10 to convert them to floats\n",
    "    class_id = [i/len(clss_map) if i != None else i for i in class_id]\n",
    "    for poly, cls_id in zip(polygons, class_id):\n",
    "        if poly:  # Ensure the polygon is not empty\n",
    "            # continues if class_id is none\n",
    "            if cls_id is None:\n",
    "                continue\n",
    "            # converts poly to a list of tuples\n",
    "            coords = [(x, y) for x, y in poly]\n",
    "            # creates Image with inverted image_size\n",
    "            img = Image.new('L', image_size, 0)\n",
    "            ImageDraw.Draw(img).polygon(coords, outline=1, fill=1)\n",
    "            # show mask\n",
    "            # Image.fromarray(np.array(img)*255).show()\n",
    "            # combines img and mask but if a pixel value is less than the new value, exept for 0, keep the old value\n",
    "            mask = np.where((np.array(img)* int(cls_id * 255) > 0) & (mask == 0), np.array(img) * int(cls_id * 255), mask)\n",
    "            mask = np.where((mask > np.array(img)* int(cls_id * 255)) & (np.array(img)* int(cls_id * 255) != 0), np.array(img) * int(cls_id * 255), mask)\n",
    "            \n",
    "            # mask = np.minimum(mask, np.array(img) * int(cls_id * 255))\n",
    "\n",
    "            # combines img and mask\n",
    "            # mask += (np.array(img)*int(cls_id*255))\n",
    "            # mask = np.maximum(mask, np.array(img))  # Combine masks\n",
    "        else:\n",
    "            raise ValueError(f\"Error: Empty polygon in class {cls_id}\")\n",
    "    # Image.fromarray(mask).show()\n",
    "    return mask\n",
    "\n",
    "def process_json_files(input_dir, img_dir, output_dir, class_map, class_map_final):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # creates and saves a class to pixel map to a csv file\n",
    "    class_map = {v: k for k, v in class_map.items()}\n",
    "    with open(os.path.join(output_dir, 'class_map.csv'), 'w') as f:\n",
    "        # adds a header to the csv file\n",
    "        f.write(\"class_id,class_name,pixel_val\\n\")\n",
    "        # f.write(\"0,background,0\\n\")\n",
    "        for count, name in class_map_final.items():\n",
    "            # gets positional value for class_map for the current class_map_final class\n",
    "            map_nme = class_map.get(name)\n",
    "            if map_nme == None:\n",
    "                if count == 0:\n",
    "                    f.write(\"0,background,0\\n\")\n",
    "                else:\n",
    "                    f.write(f\"{count},{name},{None}\\n\")\n",
    "            else:\n",
    "                f.write(f\"{count},{name},{str(int(((int(map_nme)+1)/len(class_map))*255))}\\n\")\n",
    "        # for k, v in class_map.items():\n",
    "            # f.write(f\"{v+1},{k},{str(int(((int(v)+1)/len(class_map))*255))}\\n\")\n",
    "        \n",
    "    for file in tqdm(os.listdir(input_dir), desc=\"Processing files\"):\n",
    "        if file.endswith('.json'):\n",
    "            json_path = os.path.join(input_dir, file)\n",
    "            with open(json_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            polygons, class_id = [], []\n",
    "            for annotation in data.get('annotations', []):\n",
    "                class_id.append(annotation.get('class_id'))\n",
    "                polygons.append(annotation.get('coordinates'))\n",
    "            \n",
    "            # gets image size from image file\n",
    "            img_path = os.path.join(img_dir, file.split('.')[0]+'.jpg')\n",
    "            img = Image.open(img_path)\n",
    "            # show image\n",
    "            image_size = img.size\n",
    "    \n",
    "            # Determine image size from metadata or annotations\n",
    "            # image_size = tuple(data.get('image_size', ()))  # Expecting (height, width)\n",
    "            # if not image_size:\n",
    "            #     print(f\"Error: Missing image size in {json_path}\")\n",
    "            #     continue\n",
    "\n",
    "            # replaces class 4 with null\n",
    "            class_id = [None if i == 4 else i for i in class_id]\n",
    "            # moves classes above 4 down by 1\n",
    "            class_id = [i-1 if i != None and i > 4 else i for i in class_id]\n",
    "            # class_id = [i - 1 if isinstance(i, int) and i > 4 else i for i in class_id]\n",
    "            combined_mask = create_binary_mask(polygons, image_size, class_id, class_map)\n",
    "            # combined_mask = (binary_mask * 255).astype(np.uint8)  # Ensure binary mask is 0 or 255\n",
    "            # show all unique values in mask\n",
    "            # print(np.unique(combined_mask))\n",
    "\n",
    "            # increases the \n",
    "            # show mask\n",
    "            # Image.fromarray(combined_mask).show()\n",
    "            \n",
    "            output_path = os.path.join(output_dir, f\"{os.path.splitext(file)[0]}.png\")\n",
    "            Image.fromarray(combined_mask).save(output_path)\n",
    "\n",
    "# class map of classes with associated pixel values. this version is for only positive classes with associated pixel values (no background or parent classes)\n",
    "class_map = {0: 'composite', 1: 'enamel', 2: 'pulp', 3: 'dentin', 4: 'upper', 5: 'lower'}\n",
    "# class map that orders the final class layout in relation to their class_map values\n",
    "class_map_final = {0:'background', 1: 'upper', 2: 'lower', 3: 'tooth', 4: 'pulp', 5: 'dentin', 6: 'enamel', 7: 'composite'}\n",
    "# Example usage\n",
    "input_directory = r\"I:\\Datasets\\Hierarchical Datasets\\TL_pano\\annotated\\labels\"\n",
    "img_directory = r\"I:\\Datasets\\Hierarchical Datasets\\TL_pano\\annotated\\images\"\n",
    "output_directory = r\"I:\\Datasets\\Hierarchical Datasets\\TL_pano\\processed\"\n",
    "\n",
    "process_json_files(input_directory, img_directory, output_directory, class_map, class_map_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Total paired samples: 197\n",
      "Test set: 19\n",
      "Fold 1: train=142, val=36\n",
      "Fold 2: train=142, val=36\n",
      "Fold 3: train=142, val=36\n",
      "Fold 4: train=143, val=35\n",
      "Fold 5: train=143, val=35\n"
     ]
    }
   ],
   "source": [
    "# 5 fold split\n",
    "\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "IMG_DIR   = r\"D:\\\\Datasets Main\\\\5. tooth layer segmentation\\\\TL_pano\\\\semantic\\\\images\"\n",
    "LBL_DIR   = r\"D:\\\\Datasets Main\\\\5. tooth layer segmentation\\\\TL_pano\\\\semantic\\\\labels\"\n",
    "\n",
    "OUT_DIR   = r\"D:\\\\Datasets Main\\\\5. tooth layer segmentation\\\\TL_pano\\\\semantic\\\\5_fold\"\n",
    "\n",
    "# Splitting behaviour\n",
    "INCLUDE_TEST   = True\n",
    "TEST_FRACTION  = 0.10\n",
    "FOLDS          = 5\n",
    "\n",
    "# Single-split train/val sizes (used only when not doing K-fold)\n",
    "SINGLE_TRAIN_FRACTION = 0.80\n",
    "\n",
    "IMAGE_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\"}\n",
    "LABEL_EXT  = \".png\"\n",
    "\n",
    "# rand seed\n",
    "SEED = 42\n",
    "\n",
    "# True, remove existing OUT_DIR.\n",
    "# False, overwrites dir files.\n",
    "CLEAR_OUT_DIR = True\n",
    "\n",
    "\n",
    "\n",
    "def ensure_empty_dir(p: Path):\n",
    "    if p.exists() and CLEAR_OUT_DIR:\n",
    "        shutil.rmtree(p)\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# matches images and labels\n",
    "def pair_images_and_labels(img_dir: Path, lbl_dir: Path):\n",
    "    # Index images by stem (prefer one file per stem if multiple exts exist)\n",
    "    stem_to_img = {}\n",
    "    for p in img_dir.iterdir():\n",
    "        if p.is_file() and p.suffix.lower() in IMAGE_EXTS:\n",
    "            stem = p.stem\n",
    "            # If multiple images share the same stem with different extensions, keep the first encountered.\n",
    "            stem_to_img.setdefault(stem, p)\n",
    "\n",
    "    stem_to_lbl = {}\n",
    "    for p in lbl_dir.iterdir():\n",
    "        if p.is_file() and p.suffix.lower() == LABEL_EXT:\n",
    "            stem_to_lbl[p.stem] = p\n",
    "\n",
    "    # Keep only stems that have both\n",
    "    stems = sorted(list(set(stem_to_img.keys()) & set(stem_to_lbl.keys())))\n",
    "    return stems, stem_to_img, stem_to_lbl\n",
    "\n",
    "# randomises and splits dataset into per fold sets\n",
    "def split_indices(n_total, test_frac, folds, rng):\n",
    "    indices = np.arange(n_total)\n",
    "    rng.shuffle(indices)\n",
    "\n",
    "    n_test = int(np.floor(test_frac * n_total)) if INCLUDE_TEST else 0\n",
    "    test_idx = indices[:n_test]\n",
    "    remain_idx = indices[n_test:]\n",
    "\n",
    "    if folds is not None and folds >= 2:\n",
    "        # K-fold split over remaining indices\n",
    "        chunks = np.array_split(remain_idx, folds)\n",
    "        fold_val_lists = [chunks[k] for k in range(folds)]\n",
    "        return test_idx, fold_val_lists, None, None\n",
    "    else:\n",
    "        # Single train/val over remaining indices\n",
    "        n_remain = len(remain_idx)\n",
    "        n_train = int(np.floor(SINGLE_TRAIN_FRACTION * n_remain))\n",
    "        single_train_idx = remain_idx[:n_train]\n",
    "        single_val_idx = remain_idx[n_train:]\n",
    "        return test_idx, None, single_train_idx, single_val_idx\n",
    "\n",
    "\n",
    "# copy of image and annotation files into corresponding subfolders\n",
    "def copy_pairs(stems, stem_to_img, stem_to_lbl, dst_images: Path, dst_labels: Path):\n",
    "    dst_images.mkdir(parents=True, exist_ok=True)\n",
    "    dst_labels.mkdir(parents=True, exist_ok=True)\n",
    "    for s in stems:\n",
    "        src_img = stem_to_img[s]\n",
    "        src_lbl = stem_to_lbl[s]\n",
    "        shutil.copy2(src_img, dst_images / src_img.name)\n",
    "        shutil.copy2(src_lbl, dst_labels / src_lbl.name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "img_dir = Path(IMG_DIR)\n",
    "lbl_dir = Path(LBL_DIR)\n",
    "out_dir = Path(OUT_DIR)\n",
    "\n",
    "# Basic sanity checks\n",
    "if not img_dir.exists():\n",
    "    raise FileNotFoundError(f\"Image directory not found: {img_dir}\")\n",
    "if not lbl_dir.exists():\n",
    "    raise FileNotFoundError(f\"Label directory not found: {lbl_dir}\")\n",
    "\n",
    "# Prepare output root\n",
    "ensure_empty_dir(out_dir)\n",
    "\n",
    "# Build paired dataset\n",
    "stems, stem_to_img, stem_to_lbl = pair_images_and_labels(img_dir, lbl_dir)\n",
    "if len(stems) == 0:\n",
    "    raise RuntimeError(\"No paired image/label files found. Check paths and extensions.\")\n",
    "\n",
    "rng = np.random.default_rng(SEED)\n",
    "\n",
    "test_idx, fold_val_lists, single_train_idx, single_val_idx = split_indices(\n",
    "    n_total=len(stems),\n",
    "    test_frac=TEST_FRACTION,\n",
    "    folds=FOLDS if FOLDS is not None and FOLDS >= 2 else None,\n",
    "    rng=rng,\n",
    ")\n",
    "\n",
    "# Always write test set if requested\n",
    "if INCLUDE_TEST and len(test_idx) > 0:\n",
    "    test_dir = out_dir / \"test\"\n",
    "    (test_dir / \"images\").mkdir(parents=True, exist_ok=True)\n",
    "    (test_dir / \"labels\").mkdir(parents=True, exist_ok=True)\n",
    "    test_stems = [stems[i] for i in test_idx]\n",
    "    copy_pairs(test_stems, stem_to_img, stem_to_lbl, test_dir / \"images\", test_dir / \"labels\")\n",
    "\n",
    "# K-folds or single split\n",
    "if fold_val_lists is not None:\n",
    "    # K-fold CV on the remaining data\n",
    "    remain_set = set(range(len(stems))) - set(test_idx.tolist())\n",
    "    remain_idx = np.array(sorted(list(remain_set), key=int))\n",
    "    # Build mapping\n",
    "    remain_set_global = set(remain_idx.tolist())\n",
    "\n",
    "    for k, val_idx_chunk in enumerate(fold_val_lists, start=1):\n",
    "        fold_dir = out_dir / f\"fold_{k}\"\n",
    "        train_dir = fold_dir / \"train\"\n",
    "        val_dir = fold_dir / \"val\"\n",
    "        (train_dir / \"images\").mkdir(parents=True, exist_ok=True)\n",
    "        (train_dir / \"labels\").mkdir(parents=True, exist_ok=True)\n",
    "        (val_dir / \"images\").mkdir(parents=True, exist_ok=True)\n",
    "        (val_dir / \"labels\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Validation stems for this fold\n",
    "        val_stems = [stems[i] for i in val_idx_chunk]\n",
    "\n",
    "        # Training stems are the remaining (non-test, non-val for this fold)\n",
    "        train_global_idx = list(remain_set_global - set(val_idx_chunk.tolist()))\n",
    "        train_stems = [stems[i] for i in sorted(train_global_idx, key=int)]\n",
    "\n",
    "        copy_pairs(train_stems, stem_to_img, stem_to_lbl, train_dir / \"images\", train_dir / \"labels\")\n",
    "        copy_pairs(val_stems,   stem_to_img, stem_to_lbl, val_dir / \"images\",   val_dir / \"labels\")\n",
    "\n",
    "else:\n",
    "    # Single train/val split\n",
    "    single_dir = out_dir \n",
    "    train_dir = single_dir / \"train\"\n",
    "    val_dir   = single_dir / \"val\"\n",
    "    (train_dir / \"images\").mkdir(parents=True, exist_ok=True)\n",
    "    (train_dir / \"labels\").mkdir(parents=True, exist_ok=True)\n",
    "    (val_dir / \"images\").mkdir(parents=True, exist_ok=True)\n",
    "    (val_dir / \"labels\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    train_stems = [stems[i] for i in single_train_idx]\n",
    "    val_stems   = [stems[i] for i in single_val_idx]\n",
    "    copy_pairs(train_stems, stem_to_img, stem_to_lbl, train_dir / \"images\", train_dir / \"labels\")\n",
    "    copy_pairs(val_stems,   stem_to_img, stem_to_lbl, val_dir / \"images\",   val_dir / \"labels\")\n",
    "\n",
    "# Simple summary\n",
    "n_test = len(test_idx) if INCLUDE_TEST else 0\n",
    "if fold_val_lists is not None:\n",
    "    print(f\"Done. Total paired samples: {len(stems)}\")\n",
    "    print(f\"Test set: {n_test}\")\n",
    "    for k, val_idx_chunk in enumerate(fold_val_lists, start=1):\n",
    "        remain_count = len(stems) - n_test\n",
    "        val_count = len(val_idx_chunk)\n",
    "        train_count = remain_count - val_count\n",
    "        print(f\"Fold {k}: train={train_count}, val={val_count}\")\n",
    "else:\n",
    "    print(f\"Done. Total paired samples: {len(stems)}\")\n",
    "    print(f\"Test set: {n_test}\")\n",
    "    print(f\"Train: {len(single_train_idx)}, Val: {len(single_val_idx)}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 5 folds. Test set images: 19\n",
      "fold_1: train=142, val=36\n",
      "fold_2: train=142, val=36\n",
      "fold_3: train=142, val=36\n",
      "fold_4: train=143, val=35\n",
      "fold_5: train=143, val=35\n",
      "No overlap between fold_1 val and fold_2 val\n",
      "No overlap between fold_1 val and fold_3 val\n",
      "No overlap between fold_1 val and fold_4 val\n",
      "No overlap between fold_1 val and fold_5 val\n",
      "No overlap between fold_2 val and fold_3 val\n",
      "No overlap between fold_2 val and fold_4 val\n",
      "No overlap between fold_2 val and fold_5 val\n",
      "No overlap between fold_3 val and fold_4 val\n",
      "No overlap between fold_3 val and fold_5 val\n",
      "No overlap between fold_4 val and fold_5 val\n",
      "No overlap between fold_1 train and fold_1 val\n",
      "No overlap between fold_1 train and test\n",
      "No overlap between fold_1 val and test\n",
      "No overlap between fold_2 train and fold_2 val\n",
      "No overlap between fold_2 train and test\n",
      "No overlap between fold_2 val and test\n",
      "No overlap between fold_3 train and fold_3 val\n",
      "No overlap between fold_3 train and test\n",
      "No overlap between fold_3 val and test\n",
      "No overlap between fold_4 train and fold_4 val\n",
      "No overlap between fold_4 train and test\n",
      "No overlap between fold_4 val and test\n",
      "No overlap between fold_5 train and fold_5 val\n",
      "No overlap between fold_5 train and test\n",
      "No overlap between fold_5 val and test\n",
      "No overlap between ALL-TRAIN-UNION and TEST\n",
      "No overlap between ALL-VAL-UNION and TEST\n",
      "All images in fold_1/train have labels\n",
      "No orphan labels in fold_1/train\n",
      "All images in fold_1/val have labels\n",
      "No orphan labels in fold_1/val\n",
      "All images in fold_2/train have labels\n",
      "No orphan labels in fold_2/train\n",
      "All images in fold_2/val have labels\n",
      "No orphan labels in fold_2/val\n",
      "All images in fold_3/train have labels\n",
      "No orphan labels in fold_3/train\n",
      "All images in fold_3/val have labels\n",
      "No orphan labels in fold_3/val\n",
      "All images in fold_4/train have labels\n",
      "No orphan labels in fold_4/train\n",
      "All images in fold_4/val have labels\n",
      "No orphan labels in fold_4/val\n",
      "All images in fold_5/train have labels\n",
      "No orphan labels in fold_5/train\n",
      "All images in fold_5/val have labels\n",
      "No orphan labels in fold_5/val\n",
      "All images in test have labels\n",
      "No orphan labels in test\n",
      "All checks passed: no overlaps between splits and label pairing is consistent.\n"
     ]
    }
   ],
   "source": [
    "# checks each dataset, so all fold train and test sets are unique to one another\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import re, shutil\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "OUT_DIR = Path(r\"D:\\\\Datasets Main\\\\5. tooth layer segmentation\\\\TL_pano\\\\semantic\\\\5_fold\")\n",
    "\n",
    "IMAGE_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\"}\n",
    "LABEL_EXT  = \".png\"\n",
    "RAISE_ON_OVERLAP = True\n",
    "\n",
    "\n",
    "\n",
    "# gets sorted list of base names\n",
    "def stems_in_images_dir(images_dir: Path):\n",
    "    if not images_dir.exists():\n",
    "        return []\n",
    "    return sorted([p.stem for p in images_dir.iterdir() if p.is_file() and p.suffix.lower() in IMAGE_EXTS])\n",
    "\n",
    "# gets missing labels and orphan labels as a sorted list\n",
    "def check_labels_match(images_dir: Path, labels_dir: Path, label_ext: str = LABEL_EXT):\n",
    "    img_stems = set(stems_in_images_dir(images_dir))\n",
    "    lbl_stems = set(p.stem for p in labels_dir.iterdir() \n",
    "                    if p.is_file() and p.suffix.lower() == label_ext.lower()) if labels_dir.exists() else set()\n",
    "    missing_labels = sorted(img_stems - lbl_stems)\n",
    "    orphan_labels  = sorted(lbl_stems - img_stems)\n",
    "    return missing_labels, orphan_labels\n",
    "\n",
    "def print_overlap(name_a, set_a, name_b, set_b):\n",
    "    inter = set_a & set_b\n",
    "    if inter:\n",
    "        print(f\"OVERLAP between {name_a} and {name_b}: {len(inter)} items\")\n",
    "        for s in list(sorted(inter))[:10]:\n",
    "            print(\"   -\", s)\n",
    "    else:\n",
    "        print(f\"No overlap between {name_a} and {name_b}\")\n",
    "    return inter\n",
    "\n",
    "\n",
    "fold_dirs = sorted([p for p in OUT_DIR.iterdir() if p.is_dir() and re.fullmatch(r\"fold_\\d+\", p.name)],\n",
    "                   key=lambda p: int(p.name.split(\"_\")[1]))\n",
    "\n",
    "if not fold_dirs:\n",
    "    raise RuntimeError(f\"No fold_* directories found under {OUT_DIR}. \"\n",
    "                       \"If you did a single split, adapt the checker manually.\")\n",
    "\n",
    "test_images_dir = OUT_DIR / \"test\" / \"images\"\n",
    "test_labels_dir = OUT_DIR / \"test\" / \"labels\"\n",
    "test_stems = set(stems_in_images_dir(test_images_dir))\n",
    "print(f\"Detected {len(fold_dirs)} folds. Test set images: {len(test_stems)}\")\n",
    "\n",
    "fold_data = {}\n",
    "for fd in fold_dirs:\n",
    "    train_images = fd / \"train\" / \"images\"\n",
    "    train_labels = fd / \"train\" / \"labels\"\n",
    "    val_images   = fd / \"val\"   / \"images\"\n",
    "    val_labels   = fd / \"val\"   / \"labels\"\n",
    "\n",
    "    train_stems = set(stems_in_images_dir(train_images))\n",
    "    val_stems   = set(stems_in_images_dir(val_images))\n",
    "\n",
    "    fold_data[fd.name] = {\n",
    "        \"train_images\": train_images, \"train_labels\": train_labels, \"train_stems\": train_stems,\n",
    "        \"val_images\":   val_images,   \"val_labels\":   val_labels,   \"val_stems\":   val_stems,\n",
    "    }\n",
    "\n",
    "for k, d in fold_data.items():\n",
    "    print(f\"{k}: train={len(d['train_stems'])}, val={len(d['val_stems'])}\")\n",
    "\n",
    "\n",
    "violations = []\n",
    "fold_names = list(fold_data.keys())\n",
    "for i in range(len(fold_names)):\n",
    "    for j in range(i+1, len(fold_names)):\n",
    "        fi, fj = fold_names[i], fold_names[j]\n",
    "        inter = print_overlap(f\"{fi} val\", fold_data[fi][\"val_stems\"], f\"{fj} val\", fold_data[fj][\"val_stems\"])\n",
    "        if inter:\n",
    "            violations.append((\"val_vs_val\", fi, fj, inter))\n",
    "\n",
    "for fname, d in fold_data.items():\n",
    "    inter_tv  = print_overlap(f\"{fname} train\", d[\"train_stems\"], f\"{fname} val\", d[\"val_stems\"])\n",
    "    inter_tt  = print_overlap(f\"{fname} train\", d[\"train_stems\"], \"test\", test_stems)\n",
    "    inter_vt  = print_overlap(f\"{fname} val\", d[\"val_stems\"], \"test\", test_stems)\n",
    "    if inter_tv: violations.append((\"train_vs_val\", fname, fname, inter_tv))\n",
    "    if inter_tt: violations.append((\"train_vs_test\", fname, \"test\", inter_tt))\n",
    "    if inter_vt: violations.append((\"val_vs_test\",   fname, \"test\", inter_vt))\n",
    "\n",
    "all_train_union = set().union(*[d[\"train_stems\"] for d in fold_data.values()])\n",
    "all_val_union   = set().union(*[d[\"val_stems\"]   for d in fold_data.values()])\n",
    "_ = print_overlap(\"ALL-TRAIN-UNION\", all_train_union, \"TEST\", test_stems)\n",
    "_ = print_overlap(\"ALL-VAL-UNION\",   all_val_union,   \"TEST\", test_stems)\n",
    "\n",
    "if (len(all_val_union) > 0) and (sum(len(d[\"val_stems\"]) for d in fold_data.values()) != len(all_val_union)):\n",
    "    print(\"Warning: Sum of per-fold val sizes != size of union(all val). There may be overlap.\")\n",
    "\n",
    "def report_pairing(name, img_dir, lbl_dir):\n",
    "    missing, orphan = check_labels_match(img_dir, lbl_dir, label_ext=LABEL_EXT)\n",
    "    if missing:\n",
    "        print(f\"Missing labels for images in {name}: {len(missing)} (showing up to 10)\")\n",
    "        for s in missing[:10]:\n",
    "            print(\"   -\", s + \" (no label)\")\n",
    "    else:\n",
    "        print(f\"All images in {name} have labels\")\n",
    "\n",
    "    if orphan:\n",
    "        print(f\"Labels without matching images in {name}: {len(orphan)} (showing up to 10)\")\n",
    "        for s in orphan[:10]:\n",
    "            print(\"   -\", s + \" (orphan label)\")\n",
    "    else:\n",
    "        print(f\"No orphan labels in {name}\")\n",
    "\n",
    "for fname, d in fold_data.items():\n",
    "    report_pairing(f\"{fname}/train\", d[\"train_images\"], d[\"train_labels\"])\n",
    "    report_pairing(f\"{fname}/val\",   d[\"val_images\"],   d[\"val_labels\"])\n",
    "\n",
    "if test_images_dir.exists():\n",
    "    report_pairing(\"test\", test_images_dir, test_labels_dir)\n",
    "\n",
    "\n",
    "if violations and RAISE_ON_OVERLAP:\n",
    "    msg_lines = [\"Overlap violations detected:\"]\n",
    "    for kind, a, b, inter in violations:\n",
    "        msg_lines.append(f\" - {kind}: {a} vs {b} -> {len(inter)} overlapping items (e.g. {', '.join(sorted(list(inter))[:5])})\")\n",
    "    raise AssertionError(\"\\n\".join(msg_lines))\n",
    "else:\n",
    "    if violations:\n",
    "        print(\"Completed with overlaps detected (RAISE_ON_OVERLAP=False). See messages above.\")\n",
    "    else:\n",
    "        print(\"All checks passed: no overlaps between splits and label pairing is consistent.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
