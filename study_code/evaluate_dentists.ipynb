{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b22739ad",
   "metadata": {},
   "source": [
    "### run this code to generate confusion matricies for each dentist for each class for each dentist, against the generated ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68023f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Wrote per-user confusion matrices to: F:/Github Repos/study/study_code/processed_data\\confusion_matrices_by_annotator\n"
     ]
    }
   ],
   "source": [
    "ground_truth_loc = \"F:/Github Repos/study/study_code/processed_data/task_1_inferred_ground_truth.csv\"\n",
    "\n",
    "dentists_loc = \"F:/Github Repos/study/study_code/processed_data/master_copy_task_1_users_v1_with_manual_corrections.csv\"\n",
    "\n",
    "import os\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "OUT_DIR = os.path.join(os.path.dirname(dentists_loc), \"confusion_matrices_by_annotator\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "KEYS = [\"mesial\", \"distal\", \"mesial_pls\", \"distal_pls\", \"furcation\", \"arr_left\", \"arr_right\"]\n",
    "\n",
    "MESIAL_DISTAL_KEYS = {\"mesial\", \"distal\"}\n",
    "PLS_KEYS = {\"mesial_pls\", \"distal_pls\"}\n",
    "ARR_KEYS = {\"arr_left\", \"arr_right\"}\n",
    "FURCATION_KEYS = {\"furcation\"}\n",
    "\n",
    "MESIAL_DISTAL_CLASSES = [\"Healthy\", \"Mild\", \"Moderate\", \"Severe\"]\n",
    "BINARY_CLASSES = [False, True]\n",
    "\n",
    "\n",
    "def to_binary_md(v):\n",
    "    \"\"\"\n",
    "    Mesial/Distal binarisation:\n",
    "    Healthy -> False\n",
    "    Anything else -> True\n",
    "    \"\"\"\n",
    "    if v == \"Healthy\":\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def safe_literal_eval(x):\n",
    "    if pd.isna(x):\n",
    "        return None\n",
    "    if isinstance(x, (dict, list)):\n",
    "        return x\n",
    "    s = str(x).strip()\n",
    "    if s == \"\" or s.lower() in {\"none\", \"nan\"}:\n",
    "        return None\n",
    "    try:\n",
    "        return ast.literal_eval(s)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def normalise_none_token(v):\n",
    "    if v is None:\n",
    "        return None\n",
    "    if isinstance(v, float) and np.isnan(v):\n",
    "        return None\n",
    "    if isinstance(v, str) and v.strip().lower() in {\"none\", \"nan\", \"\"}:\n",
    "        return None\n",
    "    return v\n",
    "\n",
    "def normalise_boolish(v):\n",
    "    v = normalise_none_token(v)\n",
    "    if v is None:\n",
    "        return None\n",
    "    if isinstance(v, bool):\n",
    "        return v\n",
    "    if isinstance(v, (int, np.integer)) and v in (0, 1):\n",
    "        return bool(v)\n",
    "    if isinstance(v, str):\n",
    "        s = v.strip().lower()\n",
    "        if s in {\"true\", \"t\", \"1\", \"yes\", \"y\", \"present\", \"pos\", \"positive\"}:\n",
    "            return True\n",
    "        if s in {\"false\", \"f\", \"0\", \"no\", \"n\", \"absent\", \"neg\", \"negative\"}:\n",
    "            return False\n",
    "    return v\n",
    "\n",
    "def drop_conf_keys(d):\n",
    "    if not isinstance(d, dict):\n",
    "        return d\n",
    "    return {k: v for k, v in d.items() if \"_conf\" not in str(k)}\n",
    "\n",
    "def extract_tooth_entries(obj):\n",
    "\n",
    "    obj = safe_literal_eval(obj)\n",
    "    if obj is None:\n",
    "        return []\n",
    "\n",
    "    if isinstance(obj, dict) and \"tooth_annotations\" in obj:\n",
    "        obj = obj[\"tooth_annotations\"]\n",
    "\n",
    "    out = []\n",
    "\n",
    "    if isinstance(obj, list):\n",
    "        for item in obj:\n",
    "            if not isinstance(item, dict):\n",
    "                continue\n",
    "            item = drop_conf_keys(item)\n",
    "            tn = normalise_none_token(item.get(\"tooth_num\", None))\n",
    "            if tn is None:\n",
    "                continue\n",
    "            if isinstance(tn, str) and tn.strip().isdigit():\n",
    "                tn = int(tn.strip())\n",
    "            out.append((tn, item))\n",
    "        return out\n",
    "\n",
    "    if isinstance(obj, dict):\n",
    "        if \"tooth_num\" in obj:\n",
    "            payload = drop_conf_keys(obj)\n",
    "            tn = normalise_none_token(payload.get(\"tooth_num\", None))\n",
    "            if tn is None:\n",
    "                return []\n",
    "            if isinstance(tn, str) and tn.strip().isdigit():\n",
    "                tn = int(tn.strip())\n",
    "            return [(tn, payload)]\n",
    "\n",
    "        for k, v in obj.items():\n",
    "            if not isinstance(v, dict):\n",
    "                continue\n",
    "            tn = k\n",
    "            if isinstance(tn, str) and tn.strip().isdigit():\n",
    "                tn = int(tn.strip())\n",
    "            payload = drop_conf_keys(v)\n",
    "            out.append((tn, payload))\n",
    "        return out\n",
    "\n",
    "    return []\n",
    "\n",
    "\n",
    "def should_count_pair(gt_val, dent_val, key):\n",
    "    gt_val = normalise_none_token(gt_val)\n",
    "    dent_val = normalise_none_token(dent_val)\n",
    "\n",
    "    if key in MESIAL_DISTAL_KEYS:\n",
    "        if gt_val is None:\n",
    "            gt_val = \"Healthy\"\n",
    "        if dent_val is None:\n",
    "            dent_val = \"Healthy\"\n",
    "        return True, gt_val, dent_val\n",
    "\n",
    "\n",
    "    if key in FURCATION_KEYS:\n",
    "        if gt_val is None:\n",
    "            return False, None, None\n",
    "\n",
    "        gt_b = normalise_boolish(gt_val)\n",
    "        dent_b = normalise_boolish(dent_val)\n",
    "\n",
    "        if gt_b is None:\n",
    "            return False, None, None\n",
    "\n",
    "        if dent_b is None:\n",
    "            dent_b = False\n",
    "\n",
    "        return True, gt_b, dent_b\n",
    "\n",
    "    if key in ARR_KEYS or key in PLS_KEYS:\n",
    "        gt_b = normalise_boolish(gt_val)\n",
    "        dent_b = normalise_boolish(dent_val)\n",
    "\n",
    "        if gt_b is None:\n",
    "            gt_b = False\n",
    "        if dent_b is None:\n",
    "            dent_b = False\n",
    "\n",
    "        return True, gt_b, dent_b\n",
    "\n",
    "    if gt_val is None:\n",
    "        return False, None, None\n",
    "    if dent_val is None:\n",
    "        return False, None, None\n",
    "    return True, gt_val, dent_val\n",
    "\n",
    "\n",
    "def make_confusion_df(y_true, y_pred, labels):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    return pd.DataFrame(\n",
    "        cm,\n",
    "        index=[f\"GT_{l}\" for l in labels],\n",
    "        columns=[f\"PRED_{l}\" for l in labels],\n",
    "    )\n",
    "\n",
    "\n",
    "gt_raw = pd.read_csv(ground_truth_loc)\n",
    "dent_raw = pd.read_csv(dentists_loc)\n",
    "\n",
    "if \"tooth_annotations\" not in gt_raw.columns:\n",
    "    raise ValueError(\"Ground truth CSV must contain 'tooth_annotations'.\")\n",
    "gt_image_col = \"image_name\" if \"image_name\" in gt_raw.columns else (\"image\" if \"image\" in gt_raw.columns else None)\n",
    "if gt_image_col is None:\n",
    "    raise ValueError(\"Ground truth CSV must contain 'image_name' or 'image'.\")\n",
    "\n",
    "if \"user_id\" not in dent_raw.columns:\n",
    "    raise ValueError(\"Dentists CSV must contain 'user_id'.\")\n",
    "dent_image_col = \"image\" if \"image\" in dent_raw.columns else (\"image_name\" if \"image_name\" in dent_raw.columns else None)\n",
    "if dent_image_col is None:\n",
    "    raise ValueError(\"Dentists CSV must contain 'image' or 'image_name'.\")\n",
    "if \"annotations\" not in dent_raw.columns:\n",
    "    raise ValueError(\"Dentists CSV must contain 'annotations'.\")\n",
    "\n",
    "\n",
    "gt_rows = []\n",
    "for _, r in gt_raw.iterrows():\n",
    "    img = r[gt_image_col]\n",
    "    for tooth_num, payload in extract_tooth_entries(r[\"tooth_annotations\"]):\n",
    "        row = {\"image_name\": img, \"tooth_num\": tooth_num}\n",
    "        for k in KEYS:\n",
    "            row[k] = normalise_none_token(payload.get(k, None))\n",
    "        gt_rows.append(row)\n",
    "\n",
    "gt_long = pd.DataFrame(gt_rows)\n",
    "if gt_long.empty:\n",
    "    raise ValueError(\"Parsed GT is empty. Check 'tooth_annotations' format.\")\n",
    "gt_long = gt_long.drop_duplicates(subset=[\"image_name\", \"tooth_num\"], keep=\"first\")\n",
    "\n",
    "\n",
    "dent_rows = []\n",
    "for _, r in dent_raw.iterrows():\n",
    "    user_id = r[\"user_id\"]\n",
    "    img = r[dent_image_col]\n",
    "    for tooth_num, payload in extract_tooth_entries(r[\"annotations\"]):\n",
    "        out = {\"user_id\": user_id, \"image_name\": img, \"tooth_num\": tooth_num}\n",
    "        for k in KEYS:\n",
    "            out[k] = normalise_none_token(payload.get(k, None))\n",
    "        dent_rows.append(out)\n",
    "\n",
    "dent_long = pd.DataFrame(dent_rows)\n",
    "if dent_long.empty:\n",
    "    raise ValueError(\"Parsed dentists is empty. Check 'annotations' format.\")\n",
    "\n",
    "\n",
    "all_users = sorted([u for u in dent_long[\"user_id\"].dropna().unique()])\n",
    "summary_rows = []\n",
    "\n",
    "for user in all_users:\n",
    "    user_dent = dent_long[dent_long[\"user_id\"] == user].copy()\n",
    "\n",
    "    user_df = gt_long.merge(\n",
    "        user_dent,\n",
    "        on=[\"image_name\", \"tooth_num\"],\n",
    "        suffixes=(\"_gt\", \"_dent\"),\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    user_out_dir = os.path.join(OUT_DIR, f\"user_{user}\")\n",
    "    os.makedirs(user_out_dir, exist_ok=True)\n",
    "\n",
    "    for key in KEYS:\n",
    "        gt_col = f\"{key}_gt\"\n",
    "        dent_col = f\"{key}_dent\"\n",
    "\n",
    "        y_true, y_pred = [], []\n",
    "        for gt_val, dent_val in zip(user_df[gt_col].tolist(), user_df[dent_col].tolist()):\n",
    "            ok, gt_v, dent_v = should_count_pair(gt_val, dent_val, key)\n",
    "            if not ok:\n",
    "                continue\n",
    "            y_true.append(gt_v)\n",
    "            y_pred.append(dent_v)\n",
    "\n",
    "        n_used = len(y_true)\n",
    "        if n_used == 0:\n",
    "            with open(os.path.join(user_out_dir, f\"{key}_NO_COUNTS.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(\n",
    "                    \"No valid pairs to count under rules.\\n\"\n",
    "                    \"Mesial/Distal: None treated as Healthy.\\n\"\n",
    "                    \"ARR+PLS: None treated as False (not skipped).\\n\"\n",
    "                    \"Furcation: None treated as False (not skipped).\\n\"\n",
    "                    \"Keys containing '_conf' are ignored.\\n\"\n",
    "                )\n",
    "            summary_rows.append({\"user_id\": user, \"key\": key, \"n_used\": 0})\n",
    "            continue\n",
    "\n",
    "        if key in MESIAL_DISTAL_KEYS:\n",
    "            labels = MESIAL_DISTAL_CLASSES\n",
    "            extra = sorted((set(y_true) | set(y_pred)) - set(labels))\n",
    "            labels = labels + extra\n",
    "        elif key in ARR_KEYS or key in PLS_KEYS or key in FURCATION_KEYS:\n",
    "            labels = BINARY_CLASSES\n",
    "            extra = sorted((set(y_true) | set(y_pred)) - set(labels), key=lambda x: str(x))\n",
    "            labels = labels + extra\n",
    "        else:\n",
    "            labels = sorted(set(y_true) | set(y_pred), key=lambda x: str(x))\n",
    "\n",
    "        cm_df = make_confusion_df(y_true, y_pred, labels=labels)\n",
    "        cm_df.to_csv(os.path.join(user_out_dir, f\"{key}_confusion_matrix.csv\"), index=True)\n",
    "        \n",
    "        if key in MESIAL_DISTAL_KEYS:\n",
    "            y_true_bin = [to_binary_md(v) for v in y_true]\n",
    "            y_pred_bin = [to_binary_md(v) for v in y_pred]\n",
    "\n",
    "            bin_labels = BINARY_CLASSES\n",
    "\n",
    "            cm_bin_df = make_confusion_df(\n",
    "                y_true_bin,\n",
    "                y_pred_bin,\n",
    "                labels=bin_labels,\n",
    "            )\n",
    "\n",
    "            cm_bin_df.to_csv(\n",
    "                os.path.join(user_out_dir, f\"{key}_binary_confusion_matrix.csv\"),\n",
    "                index=True,\n",
    "            )\n",
    "\n",
    "        summary_rows.append({\n",
    "            \"user_id\": user,\n",
    "            \"key\": key,\n",
    "            \"n_used\": n_used,\n",
    "            \"labels\": \"|\".join(map(str, labels)),\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows).sort_values([\"user_id\", \"key\"])\n",
    "summary_df.to_csv(os.path.join(OUT_DIR, \"SUMMARY_counts_and_labels.csv\"), index=False)\n",
    "\n",
    "print(f\"Done. Wrote per-user confusion matrices to: {OUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9a6f89",
   "metadata": {},
   "source": [
    "### run this code to evaluate dentists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27ebfc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Banksy\\AppData\\Local\\Temp\\ipykernel_13392\\1716921790.py:265: FutureWarning: The default value of observed=False is deprecated and will change to observed=True in a future version of pandas. Specify observed=False to silence this warning and retain the current behavior\n",
      "  pivot = df.pivot_table(index=\"dentist_id\", columns=\"group\", values=metric, aggfunc=\"first\")\n",
      "C:\\Users\\Banksy\\AppData\\Local\\Temp\\ipykernel_13392\\1716921790.py:265: FutureWarning: The default value of observed=False is deprecated and will change to observed=True in a future version of pandas. Specify observed=False to silence this warning and retain the current behavior\n",
      "  pivot = df.pivot_table(index=\"dentist_id\", columns=\"group\", values=metric, aggfunc=\"first\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Outputs written to:\n",
      "  F:\\Github Repos\\study\\study_code\\processed_data\\results_dentists\n",
      "Plots written to:\n",
      "  F:\\Github Repos\\study\\study_code\\processed_data\\results_dentists\\plots\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "def _normalise_label(s: str) -> str:\n",
    "    s = str(s)\n",
    "    if s.startswith(\"GT_\"):\n",
    "        return s[3:]\n",
    "    if s.startswith(\"PRED_\"):\n",
    "        return s[5:]\n",
    "    return s\n",
    "\n",
    "\n",
    "def load_confusion_csv(csv_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a confusion matrix CSV with a first column as GT labels (often 'Unnamed: 0'),\n",
    "    and columns for predicted labels.\n",
    "    Returns a DataFrame with index=GT labels, columns=PRED labels, values=int counts.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    gt_col = df.columns[0]\n",
    "    df = df.rename(columns={gt_col: \"GT_LABEL\"})\n",
    "    df[\"GT_LABEL\"] = df[\"GT_LABEL\"].map(_normalise_label)\n",
    "\n",
    "    pred_cols = [c for c in df.columns if c != \"GT_LABEL\"]\n",
    "    renamed = {c: _normalise_label(c) for c in pred_cols}\n",
    "    df = df.rename(columns=renamed)\n",
    "\n",
    "    df = df.set_index(\"GT_LABEL\")\n",
    "    df = df.apply(pd.to_numeric, errors=\"raise\").astype(int)\n",
    "\n",
    "    if (df.values < 0).any():\n",
    "        raise ValueError(f\"Negative counts found in {csv_path}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BinaryCounts:\n",
    "    tn: int\n",
    "    fp: int\n",
    "    fn: int\n",
    "    tp: int\n",
    "\n",
    "\n",
    "def _safe_div(num: float, den: float) -> float:\n",
    "    return float(num) / float(den) if den != 0 else np.nan\n",
    "\n",
    "\n",
    "def binary_counts_from_cm(cm: pd.DataFrame,\n",
    "                          negative_label: str = \"False\",\n",
    "                          positive_label: str = \"True\") -> BinaryCounts:\n",
    "    \"\"\"\n",
    "    Extract TN/FP/FN/TP from a 2x2 confusion matrix with GT rows and PRED cols.\n",
    "    \"\"\"\n",
    "    idx = {str(i): i for i in cm.index}\n",
    "    col = {str(c): c for c in cm.columns}\n",
    "\n",
    "    def pick(mapping: Dict[str, str], target: str) -> str:\n",
    "        if target in mapping:\n",
    "            return mapping[target]\n",
    "        for k, v in mapping.items():\n",
    "            if k.lower() == target.lower():\n",
    "                return v\n",
    "        raise KeyError(f\"Could not find label '{target}' in {list(mapping.keys())}\")\n",
    "\n",
    "    neg_i = pick(idx, negative_label)\n",
    "    pos_i = pick(idx, positive_label)\n",
    "    neg_c = pick(col, negative_label)\n",
    "    pos_c = pick(col, positive_label)\n",
    "\n",
    "    tn = int(cm.loc[neg_i, neg_c])\n",
    "    fp = int(cm.loc[neg_i, pos_c])\n",
    "    fn = int(cm.loc[pos_i, neg_c])\n",
    "    tp = int(cm.loc[pos_i, pos_c])\n",
    "    return BinaryCounts(tn=tn, fp=fp, fn=fn, tp=tp)\n",
    "\n",
    "\n",
    "def metrics_from_binary_counts(b: BinaryCounts) -> Dict[str, float]:\n",
    "    tn, fp, fn, tp = b.tn, b.fp, b.fn, b.tp\n",
    "    n = tn + fp + fn + tp\n",
    "\n",
    "    sens = _safe_div(tp, tp + fn)\n",
    "    spec = _safe_div(tn, tn + fp)\n",
    "    ppv  = _safe_div(tp, tp + fp)\n",
    "    npv  = _safe_div(tn, tn + fn)\n",
    "    f1   = _safe_div(2 * tp, 2 * tp + fp + fn)\n",
    "    acc  = _safe_div(tp + tn, n)\n",
    "    bal_acc = np.nanmean([sens, spec])\n",
    "\n",
    "    mcc_num = (tp * tn) - (fp * fn)\n",
    "    mcc_den = np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "    mcc = _safe_div(mcc_num, mcc_den)\n",
    "\n",
    "    fnr = 1.0 - sens\n",
    "    fpr = 1.0 - spec\n",
    "    misdx = 1.0 - acc\n",
    "\n",
    "    return {\n",
    "        \"n\": float(n),\n",
    "        \"tp\": float(tp), \"tn\": float(tn), \"fp\": float(fp), \"fn\": float(fn),\n",
    "        \"sensitivity\": sens,\n",
    "        \"specificity\": spec,\n",
    "        \"precision_ppv\": ppv,\n",
    "        \"npv\": npv,\n",
    "        \"f1\": f1,\n",
    "        \"accuracy\": acc,\n",
    "        \"balanced_accuracy\": bal_acc,\n",
    "        \"mcc\": mcc,\n",
    "\n",
    "        # requested rates\n",
    "        \"false_negative_rate\": fnr,\n",
    "        \"false_positive_rate\": fpr,\n",
    "\n",
    "        # kept for reference (but no longer used for plotting)\n",
    "        \"misdiagnosis_rate\": misdx,\n",
    "    }\n",
    "\n",
    "\n",
    "def metrics_multiclass(cm: pd.DataFrame) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Multiclass metrics with one-vs-rest macro precision/recall/F1 AND macro-specificity.\n",
    "    Also returns:\n",
    "      false_negative_rate = 1 - macro_recall\n",
    "      false_positive_rate = 1 - macro_specificity\n",
    "    \"\"\"\n",
    "    cmv = cm.values.astype(float)\n",
    "    n = cmv.sum()\n",
    "    acc = _safe_div(np.trace(cmv), n)\n",
    "\n",
    "    tp = np.diag(cmv)\n",
    "    pred_sum = cmv.sum(axis=0)\n",
    "    gt_sum = cmv.sum(axis=1)\n",
    "\n",
    "    fp = pred_sum - tp\n",
    "    fn = gt_sum - tp\n",
    "    tn = n - tp - fp - fn\n",
    "\n",
    "    prec = np.array([_safe_div(tp[i], pred_sum[i]) for i in range(len(tp))], dtype=float)\n",
    "    rec  = np.array([_safe_div(tp[i], gt_sum[i]) for i in range(len(tp))], dtype=float)\n",
    "    f1   = np.array([_safe_div(2 * prec[i] * rec[i], prec[i] + rec[i]) for i in range(len(tp))], dtype=float)\n",
    "    spec = np.array([_safe_div(tn[i], tn[i] + fp[i]) for i in range(len(tp))], dtype=float)\n",
    "\n",
    "    macro_prec = np.nanmean(prec)\n",
    "    macro_rec  = np.nanmean(rec)\n",
    "    macro_f1   = np.nanmean(f1)\n",
    "    macro_spec = np.nanmean(spec)\n",
    "\n",
    "    fnr = 1.0 - macro_rec\n",
    "    fpr = 1.0 - macro_spec\n",
    "\n",
    "    return {\n",
    "        \"n\": float(n),\n",
    "        \"accuracy\": acc,\n",
    "        \"macro_precision\": macro_prec,\n",
    "        \"macro_recall\": macro_rec,\n",
    "        \"macro_f1\": macro_f1,\n",
    "        \"balanced_accuracy\": macro_rec,  # mean recall across classes\n",
    "\n",
    "        # include specificity as a metric (macro one-vs-rest)\n",
    "        \"macro_specificity\": macro_spec,\n",
    "\n",
    "        # requested rates for heatmaps\n",
    "        \"false_negative_rate\": fnr,\n",
    "        \"false_positive_rate\": fpr,\n",
    "    }\n",
    "\n",
    "\n",
    "def bootstrap_binary_cis(b: BinaryCounts, n_boot: int = 2000, seed: int = 0) -> Dict[str, Tuple[float, float]]:\n",
    "    \"\"\"\n",
    "    Bootstrap CIs using multinomial resampling over the 4 cells.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    counts = np.array([b.tn, b.fp, b.fn, b.tp], dtype=int)\n",
    "    n = counts.sum()\n",
    "    if n == 0:\n",
    "        return {}\n",
    "\n",
    "    p = counts / n\n",
    "    boots = []\n",
    "    for _ in range(n_boot):\n",
    "        sample = rng.multinomial(n, p)\n",
    "        bb = BinaryCounts(tn=int(sample[0]), fp=int(sample[1]), fn=int(sample[2]), tp=int(sample[3]))\n",
    "        m = metrics_from_binary_counts(bb)\n",
    "        boots.append([\n",
    "            m[\"false_negative_rate\"],\n",
    "            m[\"false_positive_rate\"],\n",
    "            m[\"sensitivity\"],\n",
    "            m[\"specificity\"],\n",
    "            m[\"balanced_accuracy\"],\n",
    "            m[\"f1\"],\n",
    "            m[\"mcc\"],\n",
    "        ])\n",
    "\n",
    "    arr = np.asarray(boots, dtype=float)\n",
    "    keys = [\"false_negative_rate\", \"false_positive_rate\", \"sensitivity\", \"specificity\", \"balanced_accuracy\", \"f1\", \"mcc\"]\n",
    "    cis = {}\n",
    "    for i, k in enumerate(keys):\n",
    "        lo, hi = np.nanpercentile(arr[:, i], [2.5, 97.5])\n",
    "        cis[k] = (float(lo), float(hi))\n",
    "    return cis\n",
    "\n",
    "\n",
    "\n",
    "def find_annotator_folders(root: Path) -> List[Path]:\n",
    "    return [p for p in root.iterdir() if p.is_dir() and not p.name.startswith(\".\")]\n",
    "\n",
    "\n",
    "def collect_confusion_files(folder: Path, pattern: str = \"*confusion_matrix.csv\") -> List[Path]:\n",
    "    return sorted(folder.rglob(pattern))\n",
    "\n",
    "\n",
    "def infer_task_name(csv_path: Path) -> str:\n",
    "    name = csv_path.stem\n",
    "    for suffix in [\"_confusion_matrix\", \"confusion_matrix\"]:\n",
    "        if name.endswith(suffix):\n",
    "            name = name[: -len(suffix)]\n",
    "            break\n",
    "    return name.strip(\"_\")\n",
    "\n",
    "\n",
    "def aggregate_confusions(confusions: List[pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Sum confusion matrices with potentially different label sets.\n",
    "    \"\"\"\n",
    "    if not confusions:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    all_gt = sorted(set().union(*[set(c.index) for c in confusions]))\n",
    "    all_pr = sorted(set().union(*[set(c.columns) for c in confusions]))\n",
    "\n",
    "    agg = pd.DataFrame(0, index=all_gt, columns=all_pr, dtype=int)\n",
    "    for c in confusions:\n",
    "        agg.loc[c.index, c.columns] += c.astype(int)\n",
    "    return agg\n",
    "\n",
    "\n",
    "\n",
    "def plot_metric_heatmap(per_group_dentist: pd.DataFrame,\n",
    "                        metric: str,\n",
    "                        out_path: Path,\n",
    "                        title: str,\n",
    "                        group_order: List[str],\n",
    "                        group_labels: List[str],\n",
    "                        value_fmt: str = \"{:.3f}\") -> None:\n",
    "    \"\"\"\n",
    "    Heatmap with per-cell numeric annotations.\n",
    "    Rows: dentists (D1, D2, ...)\n",
    "    Cols: groups (PBL, ARR, PLS, Furcation)\n",
    "    \"\"\"\n",
    "    df = per_group_dentist.copy()\n",
    "    df = df[df[\"group\"].isin(group_order)].copy()\n",
    "    df[\"group\"] = pd.Categorical(df[\"group\"], categories=group_order, ordered=True)\n",
    "\n",
    "    pivot = df.pivot_table(index=\"dentist_id\", columns=\"group\", values=metric, aggfunc=\"first\")\n",
    "    pivot = pivot.reindex(columns=group_order).sort_index()\n",
    "\n",
    "    data = pivot.values.astype(float)\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 6), constrained_layout=True)\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    im = ax.imshow(data, aspect=\"auto\")\n",
    "\n",
    "    ax.set_yticks(np.arange(pivot.shape[0]))\n",
    "    ax.set_yticklabels(pivot.index)\n",
    "\n",
    "    ax.set_xticks(np.arange(pivot.shape[1]))\n",
    "    ax.set_xticklabels(group_labels, rotation=0)\n",
    "\n",
    "    ax.set_xlabel(\"Task\", labelpad=10)\n",
    "    ax.set_ylabel(\"Dentist\", labelpad=10)\n",
    "    ax.set_title(title)\n",
    "\n",
    "    cbar = fig.colorbar(im, ax=ax)\n",
    "\n",
    "    for i in range(pivot.shape[0]):\n",
    "        for j in range(pivot.shape[1]):\n",
    "            v = data[i, j]\n",
    "            txt = \"NA\" if np.isnan(v) else value_fmt.format(v)\n",
    "            ax.text(j, i, txt, ha=\"center\", va=\"center\")\n",
    "\n",
    "    fig.savefig(out_path, dpi=200)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "\n",
    "root = Path(\"F:/Github Repos/study/study_code/processed_data/confusion_matrices_by_annotator\")\n",
    "out_dir = Path(\"F:/Github Repos/study/study_code/processed_data/results_dentists\")\n",
    "n_boot = 2000\n",
    "seed = 0\n",
    "\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "plots_dir = out_dir / \"plots\"\n",
    "plots_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "GROUPS = {\n",
    "    \"mesial\":    {\"tasks\": [\"mesial_binary\"],                    \"type\": \"binary\"},\n",
    "    \"distal\":    {\"tasks\": [\"distal_binary\"],                    \"type\": \"binary\"},\n",
    "    \"PBL\":       {\"tasks\": [\"mesial_binary\", \"distal_binary\"],   \"type\": \"binary\"},  # mesial+distal combined (binary)\n",
    "    \"ARR\":       {\"tasks\": [\"arr_left\", \"arr_right\"],            \"type\": \"binary\"},\n",
    "    \"Furcation\": {\"tasks\": [\"furcation\"],                        \"type\": \"binary\"},\n",
    "    \"PLS\":       {\"tasks\": [\"mesial_pls\", \"distal_pls\"],         \"type\": \"binary\"},\n",
    "}\n",
    "\n",
    "# Dentist renaming to D1, D2, ...\n",
    "annotator_folders = sorted(find_annotator_folders(root), key=lambda p: p.name)\n",
    "if not annotator_folders:\n",
    "    raise RuntimeError(f\"No annotator subfolders found under: {root}\")\n",
    "\n",
    "dentist_map = {p.name: f\"D{i+1}\" for i, p in enumerate(annotator_folders)}\n",
    "\n",
    "# Load all CSVs once\n",
    "all_rows = []\n",
    "for folder in annotator_folders:\n",
    "    dentist_raw = folder.name\n",
    "    dentist_id = dentist_map[dentist_raw]\n",
    "\n",
    "    csv_files = collect_confusion_files(folder)\n",
    "    for csv_path in csv_files:\n",
    "        task = infer_task_name(csv_path)\n",
    "        cm = load_confusion_csv(csv_path)\n",
    "\n",
    "        all_rows.append({\n",
    "            \"dentist_raw\": dentist_raw,\n",
    "            \"dentist_id\": dentist_id,\n",
    "            \"task\": task,\n",
    "            \"file\": str(csv_path),\n",
    "            \"cm\": cm,\n",
    "        })\n",
    "\n",
    "all_df = pd.DataFrame(all_rows)\n",
    "if all_df.empty:\n",
    "    raise RuntimeError(f\"No confusion_matrix.csv files found under: {root}\")\n",
    "\n",
    "# Per-group outputs\n",
    "per_group_file_rows = []\n",
    "per_group_dentist_rows = []\n",
    "\n",
    "for group_name, spec in GROUPS.items():\n",
    "    tasks = set(spec[\"tasks\"])\n",
    "    group_type = spec[\"type\"]\n",
    "\n",
    "    group_out = out_dir / group_name\n",
    "    group_out.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    gdf = all_df[all_df[\"task\"].isin(tasks)].copy()\n",
    "\n",
    "    for _, r in gdf.iterrows():\n",
    "        cm = r[\"cm\"]\n",
    "\n",
    "        if group_type == \"binary\":\n",
    "            if cm.shape != (2, 2):\n",
    "                raise ValueError(f\"[{group_name}] Expected binary 2x2 but got {cm.shape} in {r['file']}\")\n",
    "            b = binary_counts_from_cm(cm, \"False\", \"True\")\n",
    "            m = metrics_from_binary_counts(b)\n",
    "        else:\n",
    "            m = metrics_multiclass(cm)\n",
    "\n",
    "        per_group_file_rows.append({\n",
    "            \"group\": group_name,\n",
    "            \"dentist_raw\": r[\"dentist_raw\"],\n",
    "            \"dentist_id\": r[\"dentist_id\"],\n",
    "            \"task\": r[\"task\"],\n",
    "            \"file\": r[\"file\"],\n",
    "            **m\n",
    "        })\n",
    "\n",
    "    for dentist_raw, sub in gdf.groupby(\"dentist_raw\"):\n",
    "        dentist_id = dentist_map[dentist_raw]\n",
    "        cms = list(sub[\"cm\"].values)\n",
    "        agg = aggregate_confusions(cms)\n",
    "\n",
    "        if agg.empty:\n",
    "            continue\n",
    "\n",
    "        if group_type == \"binary\":\n",
    "            if agg.shape != (2, 2):\n",
    "                raise ValueError(f\"[{group_name}] Expected aggregated binary 2x2 but got {agg.shape} for {dentist_raw}\")\n",
    "            b = binary_counts_from_cm(agg, \"False\", \"True\")\n",
    "            m = metrics_from_binary_counts(b)\n",
    "            cis = bootstrap_binary_cis(b, n_boot=n_boot, seed=seed)\n",
    "\n",
    "            row = {\"group\": group_name, \"dentist_raw\": dentist_raw, \"dentist_id\": dentist_id, **m}\n",
    "            for k, (lo, hi) in cis.items():\n",
    "                row[f\"{k}_ci_low\"] = lo\n",
    "                row[f\"{k}_ci_high\"] = hi\n",
    "            per_group_dentist_rows.append(row)\n",
    "        else:\n",
    "            m = metrics_multiclass(agg)\n",
    "            per_group_dentist_rows.append({\"group\": group_name, \"dentist_raw\": dentist_raw, \"dentist_id\": dentist_id, **m})\n",
    "\n",
    "    # Save group CSVs\n",
    "    per_file_g = pd.DataFrame([r for r in per_group_file_rows if r[\"group\"] == group_name])\n",
    "    per_dent_g = pd.DataFrame([r for r in per_group_dentist_rows if r[\"group\"] == group_name])\n",
    "\n",
    "    per_file_g.to_csv(group_out / f\"per_file_metrics_{group_name}.csv\", index=False)\n",
    "    per_dent_g.to_csv(group_out / f\"per_dentist_metrics_{group_name}.csv\", index=False)\n",
    "\n",
    "per_file = pd.DataFrame(per_group_file_rows)\n",
    "per_dentist = pd.DataFrame(per_group_dentist_rows)\n",
    "per_file.to_csv(out_dir / \"per_file_metrics_ALLGROUPS.csv\", index=False)\n",
    "per_dentist.to_csv(out_dir / \"per_dentist_metrics_ALLGROUPS.csv\", index=False)\n",
    "\n",
    "\n",
    "plot_groups = [\"PBL\", \"ARR\", \"PLS\", \"Furcation\"]\n",
    "plot_labels = [\"PBL (mesial+distal, binary)\", \"ARR\", \"PLS\", \"Furcation\"]\n",
    "\n",
    "plot_df = per_dentist[per_dentist[\"group\"].isin(plot_groups)].copy()\n",
    "\n",
    "plot_metric_heatmap(\n",
    "    per_group_dentist=plot_df,\n",
    "    metric=\"false_negative_rate\",\n",
    "    out_path=plots_dir / \"heatmap_false_negative_rate.png\",\n",
    "    title=\"False Negative Rate (1 - recall)\",\n",
    "    group_order=plot_groups,\n",
    "    group_labels=plot_labels,\n",
    ")\n",
    "\n",
    "plot_metric_heatmap(\n",
    "    per_group_dentist=plot_df,\n",
    "    metric=\"false_positive_rate\",\n",
    "    out_path=plots_dir / \"heatmap_false_positive_rate.png\",\n",
    "    title=\"False Positive Rate (1 - specificity)\",\n",
    "    group_order=plot_groups,\n",
    "    group_labels=plot_labels,\n",
    ")\n",
    "\n",
    "print(\"Done. Outputs written to:\")\n",
    "print(f\"  {out_dir}\")\n",
    "print(\"Plots written to:\")\n",
    "print(f\"  {plots_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f031c5af",
   "metadata": {},
   "source": [
    "### generate latex tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8076c2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}[!htbp]\n",
      "\\centering\n",
      "\\scriptsize\n",
      "\\begin{adjustbox}{center, max width=\\paperwidth}\n",
      "\\begin{tabular}{l l|cccccc}\n",
      "\\toprule\n",
      "\\multicolumn{8}{c}{Observer Evaluation}\\\\\n",
      "\\midrule\n",
      "Evaluation & Observer & Precision & Recall & F1 & Specificity & FNR & FPR \\\\\n",
      "\\midrule\n",
      "\\multirow{8}{*}{Mesial (binary)} & D1 & 1.000 & 0.444 & 0.615 & 1.000 & 0.556 & 0.000 \\\\\n",
      " & D2 & 1.000 & 0.889 & 0.941 & 1.000 & 0.111 & 0.000 \\\\\n",
      " & D3 & 0.722 & 0.722 & 0.722 & 0.545 & 0.278 & 0.455 \\\\\n",
      " & D4 & 1.000 & 0.167 & 0.286 & 1.000 & 0.833 & 0.000 \\\\\n",
      " & D5 & 0.846 & 0.611 & 0.710 & 0.818 & 0.389 & 0.182 \\\\\n",
      " & D6 & 1.000 & 0.500 & 0.667 & 1.000 & 0.500 & 0.000 \\\\\n",
      " & D7 & 1.000 & 0.944 & 0.971 & 1.000 & 0.056 & 0.000 \\\\\n",
      " & A & 0.938 & 0.611 & 0.702 & 0.909 & 0.389 & 0.091 \\\\\n",
      "\\midrule\n",
      "\\multirow{8}{*}{Distal (binary)} & D1 & 1.000 & 0.500 & 0.667 & 1.000 & 0.500 & 0.000 \\\\\n",
      " & D2 & 0.929 & 0.650 & 0.765 & 0.889 & 0.350 & 0.111 \\\\\n",
      " & D3 & 0.778 & 0.700 & 0.737 & 0.556 & 0.300 & 0.444 \\\\\n",
      " & D4 & 1.000 & 0.200 & 0.333 & 1.000 & 0.800 & 0.000 \\\\\n",
      " & D5 & 0.867 & 0.650 & 0.743 & 0.778 & 0.350 & 0.222 \\\\\n",
      " & D6 & 1.000 & 0.526 & 0.690 & 1.000 & 0.474 & 0.000 \\\\\n",
      " & D7 & 0.882 & 0.750 & 0.811 & 0.778 & 0.250 & 0.222 \\\\\n",
      " & A & 0.922 & 0.568 & 0.678 & 0.857 & 0.432 & 0.143 \\\\\n",
      "\\midrule\n",
      "\\multirow{8}{*}{PBL (mesial+distal, binary)} & D1 & 1.000 & 0.474 & 0.643 & 1.000 & 0.526 & 0.000 \\\\\n",
      " & D2 & 0.967 & 0.763 & 0.853 & 0.950 & 0.237 & 0.050 \\\\\n",
      " & D3 & 0.750 & 0.711 & 0.730 & 0.550 & 0.289 & 0.450 \\\\\n",
      " & D4 & 1.000 & 0.184 & 0.311 & 1.000 & 0.816 & 0.000 \\\\\n",
      " & D5 & 0.857 & 0.632 & 0.727 & 0.800 & 0.368 & 0.200 \\\\\n",
      " & D6 & 1.000 & 0.514 & 0.679 & 1.000 & 0.486 & 0.000 \\\\\n",
      " & D7 & 0.941 & 0.842 & 0.889 & 0.900 & 0.158 & 0.100 \\\\\n",
      " & A & 0.931 & 0.588 & 0.690 & 0.886 & 0.412 & 0.114 \\\\\n",
      "\\midrule\n",
      "\\multirow{8}{*}{ARR} & D1 & 1.000 & 0.250 & 0.400 & 1.000 & 0.750 & 0.000 \\\\\n",
      " & D2 & 1.000 & 0.500 & 0.667 & 1.000 & 0.500 & 0.000 \\\\\n",
      " & D3 & 1.000 & 0.750 & 0.857 & 1.000 & 0.250 & 0.000 \\\\\n",
      " & D4 & 1.000 & 0.625 & 0.769 & 1.000 & 0.375 & 0.000 \\\\\n",
      " & D5 & 1.000 & 0.375 & 0.545 & 1.000 & 0.625 & 0.000 \\\\\n",
      " & D6 & 1.000 & 0.375 & 0.545 & 1.000 & 0.625 & 0.000 \\\\\n",
      " & D7 & 1.000 & 0.375 & 0.545 & 1.000 & 0.625 & 0.000 \\\\\n",
      " & A & 1.000 & 0.464 & 0.618 & 1.000 & 0.536 & 0.000 \\\\\n",
      "\\midrule\n",
      "\\multirow{8}{*}{PLS} & D1 & 0.250 & 0.400 & 0.308 & 0.887 & 0.600 & 0.113 \\\\\n",
      " & D2 & 0.571 & 0.800 & 0.667 & 0.943 & 0.200 & 0.057 \\\\\n",
      " & D3 & 0.115 & 0.600 & 0.194 & 0.566 & 0.400 & 0.434 \\\\\n",
      " & D4 & 0.200 & 0.400 & 0.267 & 0.849 & 0.600 & 0.151 \\\\\n",
      " & D5 & 0.192 & 1.000 & 0.323 & 0.604 & 0.000 & 0.396 \\\\\n",
      " & D6 & 0.714 & 1.000 & 0.833 & 0.959 & 0.000 & 0.041 \\\\\n",
      " & D7 & 1.000 & 0.400 & 0.571 & 1.000 & 0.600 & 0.000 \\\\\n",
      " & A & 0.435 & 0.657 & 0.452 & 0.830 & 0.343 & 0.170 \\\\\n",
      "\\midrule\n",
      "\\multirow{8}{*}{Furcation} & D1 & 1.000 & 1.000 & 1.000 & 1.000 & 0.000 & 0.000 \\\\\n",
      " & D2 & 1.000 & 1.000 & 1.000 & 1.000 & 0.000 & 0.000 \\\\\n",
      " & D3 & 0.667 & 1.000 & 0.800 & 0.889 & 0.000 & 0.111 \\\\\n",
      " & D4 & 0.500 & 1.000 & 0.667 & 0.778 & 0.000 & 0.222 \\\\\n",
      " & D5 & 0.667 & 1.000 & 0.800 & 0.889 & 0.000 & 0.111 \\\\\n",
      " & D6 & 1.000 & 0.500 & 0.667 & 1.000 & 0.500 & 0.000 \\\\\n",
      " & D7 & 0.400 & 1.000 & 0.571 & 0.667 & 0.000 & 0.333 \\\\\n",
      " & A & 0.748 & 0.929 & 0.786 & 0.889 & 0.071 & 0.111 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{adjustbox}\n",
      "\\caption{Observer-wise results across evaluation types. Metrics are computed from aggregated confusion matrices per observer. For binary tasks, macro metrics are set to their binary equivalents. Row \\textbf{A} reports the mean across observers within each evaluation type.}\n",
      "\\label{tab:observer_eval_single}\n",
      "\\end{table}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "out_dir = Path(r\"F:/Github Repos/study/study_code/processed_data/results_dentists\")\n",
    "results_path = out_dir / \"per_dentist_metrics_ALLGROUPS.csv\"\n",
    "\n",
    "GROUP_ORDER = [\"mesial\", \"distal\", \"PBL\", \"ARR\", \"PLS\", \"Furcation\"]\n",
    "GROUP_LABELS = {\n",
    "    \"mesial\": \"Mesial (binary)\",\n",
    "    \"distal\": \"Distal (binary)\",\n",
    "    \"PBL\": \"PBL (mesial+distal, binary)\",\n",
    "    \"ARR\": \"ARR\",\n",
    "    \"PLS\": \"PLS\",\n",
    "    \"Furcation\": \"Furcation\",\n",
    "}\n",
    "\n",
    "METRICS = [\n",
    "    (\"macro_precision\", \"Precision\"),\n",
    "    (\"macro_recall\", \"Recall\"),\n",
    "    (\"macro_f1\", \"F1\"),\n",
    "    (\"macro_specificity\", \"Specificity\"),\n",
    "    (\"false_negative_rate\", \"FNR\"),\n",
    "    (\"false_positive_rate\", \"FPR\"),\n",
    "]\n",
    "\n",
    "CAPTION = (\n",
    "    \"Observer-wise results across evaluation types. Metrics are computed from aggregated confusion matrices per observer. \"\n",
    "    \"For binary tasks, macro metrics are set to their binary equivalents. Row \\\\textbf{A} reports the mean across observers \"\n",
    "    \"within each evaluation type.\"\n",
    ")\n",
    "LABEL = \"tab:observer_eval_single\"\n",
    "\n",
    "\n",
    "def _fmt(x: float, ndp: int = 3) -> str:\n",
    "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
    "        return \"NA\"\n",
    "    return f\"{float(x):.{ndp}f}\"\n",
    "\n",
    "def _latex_escape(s: str) -> str:\n",
    "    s = str(s)\n",
    "    return (s.replace(\"\\\\\", r\"\\textbackslash{}\")\n",
    "             .replace(\"&\", r\"\\&\")\n",
    "             .replace(\"%\", r\"\\%\")\n",
    "             .replace(\"$\", r\"\\$\")\n",
    "             .replace(\"#\", r\"\\#\")\n",
    "             .replace(\"_\", r\"\\_\")\n",
    "             .replace(\"{\", r\"\\{\")\n",
    "             .replace(\"}\", r\"\\}\")\n",
    "             .replace(\"~\", r\"\\textasciitilde{}\")\n",
    "             .replace(\"^\", r\"\\textasciicircum{}\"))\n",
    "\n",
    "def _observer_sort_key(s: str) -> int:\n",
    "    m = re.match(r\"D(\\d+)$\", str(s))\n",
    "    return int(m.group(1)) if m else 10**9\n",
    "\n",
    "def _is_binary_group(g: str) -> bool:\n",
    "    return g in {\"mesial\", \"distal\", \"PBL\", \"ARR\", \"PLS\", \"Furcation\"}\n",
    "\n",
    "def _autofind_results_csv(root: Path) -> Path:\n",
    "    cands = list(root.rglob(\"per_dentist_metrics_ALLGROUPS.csv\"))\n",
    "    if cands:\n",
    "        return cands[0]\n",
    "    cands = list(root.rglob(\"*per_dentist_metrics*ALLGROUPS*.csv\"))\n",
    "    if cands:\n",
    "        return cands[0]\n",
    "    raise FileNotFoundError(\n",
    "        f\"Could not find per_dentist_metrics_ALLGROUPS.csv under: {root}\\n\"\n",
    "        \"Set `results_path` to the correct CSV.\"\n",
    "    )\n",
    "\n",
    "def _ensure_macro_columns_for_binary(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Ensure required macro_* columns exist for BOTH multiclass and binary rows.\n",
    "    For binary rows, map:\n",
    "      macro_precision    <- precision_ppv\n",
    "      macro_recall       <- sensitivity\n",
    "      macro_f1           <- f1\n",
    "      macro_specificity  <- specificity\n",
    "\n",
    "    Also ensure FNR/FPR exist:\n",
    "      FNR <- 1 - macro_recall\n",
    "      FPR <- 1 - macro_specificity\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    is_bin = df[\"group\"].apply(_is_binary_group)\n",
    "\n",
    "    for c in [\"macro_precision\", \"macro_recall\", \"macro_f1\", \"macro_specificity\"]:\n",
    "        if c not in df.columns:\n",
    "            df[c] = np.nan\n",
    "\n",
    "    def fill(target: str, source: str):\n",
    "        if source in df.columns:\n",
    "            mask = is_bin & df[target].isna()\n",
    "            df.loc[mask, target] = pd.to_numeric(df.loc[mask, source], errors=\"coerce\")\n",
    "\n",
    "    fill(\"macro_precision\", \"precision_ppv\")\n",
    "    fill(\"macro_recall\", \"sensitivity\")\n",
    "    fill(\"macro_f1\", \"f1\")\n",
    "    fill(\"macro_specificity\", \"specificity\")\n",
    "\n",
    "    if \"false_negative_rate\" not in df.columns:\n",
    "        df[\"false_negative_rate\"] = 1.0 - pd.to_numeric(df[\"macro_recall\"], errors=\"coerce\")\n",
    "    if \"false_positive_rate\" not in df.columns:\n",
    "        df[\"false_positive_rate\"] = 1.0 - pd.to_numeric(df[\"macro_specificity\"], errors=\"coerce\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "if not results_path.exists():\n",
    "    results_path = _autofind_results_csv(out_dir)\n",
    "\n",
    "df = pd.read_csv(results_path)\n",
    "\n",
    "required = {\"dentist_id\", \"group\"}\n",
    "missing = required - set(df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing required columns in results CSV: {missing}\")\n",
    "\n",
    "df = df[df[\"group\"].isin(GROUP_ORDER)].copy()\n",
    "df[\"group\"] = pd.Categorical(df[\"group\"], categories=GROUP_ORDER, ordered=True)\n",
    "\n",
    "df = _ensure_macro_columns_for_binary(df)\n",
    "\n",
    "keep_cols = [\"dentist_id\", \"group\"] + [c for c, _ in METRICS]\n",
    "df = df[keep_cols].copy()\n",
    "df[\"dentist_id\"] = df[\"dentist_id\"].astype(str)\n",
    "\n",
    "observers = sorted(df[\"dentist_id\"].unique().tolist(), key=_observer_sort_key)\n",
    "row_index = observers + [\"A\"]\n",
    "\n",
    "avg_rows = []\n",
    "for g in GROUP_ORDER:\n",
    "    sub = df[df[\"group\"] == g].copy()\n",
    "    if sub.empty:\n",
    "        continue\n",
    "    row = {\"group\": g, \"dentist_id\": \"A\"}\n",
    "    for col, _ in METRICS:\n",
    "        row[col] = pd.to_numeric(sub[col], errors=\"coerce\").mean()\n",
    "    avg_rows.append(row)\n",
    "\n",
    "df = pd.concat([df, pd.DataFrame(avg_rows)], ignore_index=True)\n",
    "\n",
    "\n",
    "metric_disp = [disp for _, disp in METRICS]\n",
    "n_metrics = len(metric_disp)\n",
    "\n",
    "colspec = \"l l|\" + \"\".join([\"c\"] * n_metrics)\n",
    "\n",
    "lines = []\n",
    "lines.append(r\"\\begin{table}[!htbp]\")\n",
    "lines.append(r\"\\centering\")\n",
    "lines.append(r\"\\scriptsize\")\n",
    "lines.append(r\"\\begin{adjustbox}{center, max width=\\paperwidth}\")\n",
    "lines.append(r\"\\begin{tabular}{\" + colspec + r\"}\")\n",
    "lines.append(r\"\\toprule\")\n",
    "lines.append(r\"\\multicolumn{\" + str(2 + n_metrics) + r\"}{c}{Observer Evaluation}\\\\\")\n",
    "lines.append(r\"\\midrule\")\n",
    "\n",
    "# Header row\n",
    "hdr = [\"Evaluation\", \"Observer\"] + metric_disp\n",
    "lines.append(\" & \".join(map(_latex_escape, hdr)) + r\" \\\\\")\n",
    "lines.append(r\"\\midrule\")\n",
    "\n",
    "for gi, g in enumerate(GROUP_ORDER):\n",
    "    sub = df[df[\"group\"] == g].copy()\n",
    "    if sub.empty:\n",
    "        continue\n",
    "\n",
    "    sub = sub.set_index(\"dentist_id\")\n",
    "    group_rows = []\n",
    "    for obs in row_index:\n",
    "        if obs not in sub.index:\n",
    "            vals = [\"NA\"] * n_metrics\n",
    "        else:\n",
    "            vals = []\n",
    "            for col, _disp in METRICS:\n",
    "                vals.append(_fmt(pd.to_numeric(sub.loc[obs, col], errors=\"coerce\")))\n",
    "        group_rows.append((obs, vals))\n",
    "\n",
    "    eval_name = GROUP_LABELS.get(g, g)\n",
    "\n",
    "    # First row with multirow evaluation label\n",
    "    lines.append(\n",
    "        r\"\\multirow{\" + str(len(group_rows)) + r\"}{*}{\" + _latex_escape(eval_name) + r\"} & \"\n",
    "        + _latex_escape(group_rows[0][0]) + \" & \" + \" & \".join(group_rows[0][1]) + r\" \\\\\"\n",
    "    )\n",
    "    # Remaining rows\n",
    "    for obs, vals in group_rows[1:]:\n",
    "        lines.append(r\" & \" + _latex_escape(obs) + \" & \" + \" & \".join(vals) + r\" \\\\\")\n",
    "    # Separator\n",
    "    if gi == len(GROUP_ORDER) - 1:\n",
    "        lines.append(r\"\\bottomrule\")\n",
    "    else:\n",
    "        lines.append(r\"\\midrule\")\n",
    "\n",
    "lines.append(r\"\\end{tabular}\")\n",
    "lines.append(r\"\\end{adjustbox}\")\n",
    "lines.append(r\"\\caption{\" + CAPTION + r\"}\")\n",
    "lines.append(r\"\\label{\" + LABEL + r\"}\")\n",
    "lines.append(r\"\\end{table}\")\n",
    "\n",
    "print(\"\\n\".join(lines))\n",
    "\n",
    "# Required LaTeX packages:\n",
    "# \\usepackage{booktabs}\n",
    "# \\usepackage{multirow}\n",
    "# \\usepackage{adjustbox}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936c91b2",
   "metadata": {},
   "source": [
    "### run this code to evaluate models\n",
    "\n",
    "############################################################\n",
    "\n",
    "############################################################\n",
    "\n",
    "############################################################\n",
    "\n",
    "############################################################\n",
    "\n",
    "############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f32377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Wrote confusion matrices to: F:/Github Repos/study/study_code/processed_data/object_detection\\confusion_matrices\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Attached files in this environment\n",
    "ground_truth_loc = \"F:/Github Repos/study/study_code/processed_data/task_1_inferred_ground_truth.csv\"\n",
    "\n",
    "results_loc = \"F:/Github Repos/study/study_code/processed_data/object_detection/object_detection_results.csv\"\n",
    "\n",
    "OUT_DIR = os.path.join(os.path.dirname(results_loc), \"confusion_matrices\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "KEYS = [\"mesial\", \"distal\", \"mesial_pls\", \"distal_pls\", \"furcation\", \"arr_left\", \"arr_right\"]\n",
    "\n",
    "MESIAL_DISTAL_KEYS = {\"mesial\", \"distal\"}\n",
    "PLS_KEYS = {\"mesial_pls\", \"distal_pls\"}\n",
    "ARR_KEYS = {\"arr_left\", \"arr_right\"}\n",
    "FURCATION_KEYS = {\"furcation\"}\n",
    "\n",
    "MESIAL_DISTAL_CLASSES = [\"Healthy\", \"Mild\", \"Moderate\", \"Severe\"]\n",
    "BINARY_CLASSES = [False, True]\n",
    "\n",
    "\n",
    "def to_binary_md(v):\n",
    "    \"\"\"Mesial/Distal binarisation: Healthy -> False; Anything else -> True.\"\"\"\n",
    "    return False if v == \"Healthy\" else True\n",
    "\n",
    "\n",
    "def safe_literal_eval(x):\n",
    "    if pd.isna(x):\n",
    "        return None\n",
    "    if isinstance(x, (dict, list)):\n",
    "        return x\n",
    "    s = str(x).strip()\n",
    "    if s == \"\" or s.lower() in {\"none\", \"nan\"}:\n",
    "        return None\n",
    "    try:\n",
    "        return ast.literal_eval(s)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def normalise_none_token(v):\n",
    "    if v is None:\n",
    "        return None\n",
    "    if isinstance(v, float) and np.isnan(v):\n",
    "        return None\n",
    "    if isinstance(v, str) and v.strip().lower() in {\"none\", \"nan\", \"\"}:\n",
    "        return None\n",
    "    return v\n",
    "\n",
    "\n",
    "def normalise_boolish(v):\n",
    "    v = normalise_none_token(v)\n",
    "    if v is None:\n",
    "        return None\n",
    "    if isinstance(v, bool):\n",
    "        return v\n",
    "    if isinstance(v, (int, np.integer)) and v in (0, 1):\n",
    "        return bool(v)\n",
    "    if isinstance(v, str):\n",
    "        s = v.strip().lower()\n",
    "        if s in {\"true\", \"t\", \"1\", \"yes\", \"y\", \"present\", \"pos\", \"positive\"}:\n",
    "            return True\n",
    "        if s in {\"false\", \"f\", \"0\", \"no\", \"n\", \"absent\", \"neg\", \"negative\"}:\n",
    "            return False\n",
    "    return v  # keep unexpected labels visible\n",
    "\n",
    "\n",
    "def drop_conf_keys(d):\n",
    "    if not isinstance(d, dict):\n",
    "        return d\n",
    "    return {k: v for k, v in d.items() if \"_conf\" not in str(k)}\n",
    "\n",
    "\n",
    "def extract_tooth_entries(obj):\n",
    "    \"\"\"\n",
    "    Accepts:\n",
    "      - list[dict] with 'tooth_num'\n",
    "      - dict with 'tooth_annotations'\n",
    "      - dict keyed by tooth_num\n",
    "      - single dict with 'tooth_num'\n",
    "    Returns list[(tooth_num, payload_dict)].\n",
    "    \"\"\"\n",
    "    obj = safe_literal_eval(obj)\n",
    "    if obj is None:\n",
    "        return []\n",
    "\n",
    "    if isinstance(obj, dict) and \"tooth_annotations\" in obj:\n",
    "        obj = obj[\"tooth_annotations\"]\n",
    "\n",
    "    out = []\n",
    "\n",
    "    if isinstance(obj, list):\n",
    "        for item in obj:\n",
    "            if not isinstance(item, dict):\n",
    "                continue\n",
    "            item = drop_conf_keys(item)\n",
    "            tn = normalise_none_token(item.get(\"tooth_num\", None))\n",
    "            if tn is None:\n",
    "                continue\n",
    "            if isinstance(tn, str) and tn.strip().isdigit():\n",
    "                tn = int(tn.strip())\n",
    "            out.append((tn, item))\n",
    "        return out\n",
    "\n",
    "    if isinstance(obj, dict):\n",
    "        if \"tooth_num\" in obj:\n",
    "            payload = drop_conf_keys(obj)\n",
    "            tn = normalise_none_token(payload.get(\"tooth_num\", None))\n",
    "            if tn is None:\n",
    "                return []\n",
    "            if isinstance(tn, str) and tn.strip().isdigit():\n",
    "                tn = int(tn.strip())\n",
    "            return [(tn, payload)]\n",
    "\n",
    "        for k, v in obj.items():\n",
    "            if not isinstance(v, dict):\n",
    "                continue\n",
    "            tn = k\n",
    "            if isinstance(tn, str) and tn.strip().isdigit():\n",
    "                tn = int(tn.strip())\n",
    "            payload = drop_conf_keys(v)\n",
    "            out.append((tn, payload))\n",
    "        return out\n",
    "\n",
    "    return []\n",
    "\n",
    "\n",
    "def should_count_pair(gt_val, pred_val, key):\n",
    "    gt_val = normalise_none_token(gt_val)\n",
    "    pred_val = normalise_none_token(pred_val)\n",
    "\n",
    "    # Mesial/Distal\n",
    "    if key in MESIAL_DISTAL_KEYS:\n",
    "        if gt_val is None:\n",
    "            gt_val = \"Healthy\"\n",
    "        if pred_val is None:\n",
    "            pred_val = \"Healthy\"\n",
    "        return True, gt_val, pred_val\n",
    "\n",
    "    # Furcation\n",
    "    if key in FURCATION_KEYS:\n",
    "        if gt_val is None:\n",
    "            return False, None, None\n",
    "\n",
    "        gt_b = normalise_boolish(gt_val)\n",
    "        pred_b = normalise_boolish(pred_val)\n",
    "\n",
    "        if gt_b is None:\n",
    "            return False, None, None\n",
    "        if pred_b is None:\n",
    "            pred_b = False\n",
    "\n",
    "        return True, gt_b, pred_b\n",
    "\n",
    "    # ARR + PLS\n",
    "    if key in ARR_KEYS or key in PLS_KEYS:\n",
    "        gt_b = normalise_boolish(gt_val)\n",
    "        pred_b = normalise_boolish(pred_val)\n",
    "\n",
    "        if gt_b is None:\n",
    "            gt_b = False\n",
    "        if pred_b is None:\n",
    "            pred_b = False\n",
    "\n",
    "        return True, gt_b, pred_b\n",
    "\n",
    "    if gt_val is None or pred_val is None:\n",
    "        return False, None, None\n",
    "    return True, gt_val, pred_val\n",
    "\n",
    "\n",
    "def make_confusion_df(y_true, y_pred, labels):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    return pd.DataFrame(\n",
    "        cm,\n",
    "        index=[f\"GT_{l}\" for l in labels],\n",
    "        columns=[f\"PRED_{l}\" for l in labels],\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "gt_raw = pd.read_csv(ground_truth_loc)\n",
    "pred_raw = pd.read_csv(results_loc)\n",
    "\n",
    "# Ground truth requirements\n",
    "if \"tooth_annotations\" not in gt_raw.columns:\n",
    "    raise ValueError(\"Ground truth CSV must contain 'tooth_annotations'.\")\n",
    "gt_image_col = \"image_name\" if \"image_name\" in gt_raw.columns else (\"image\" if \"image\" in gt_raw.columns else None)\n",
    "if gt_image_col is None:\n",
    "    raise ValueError(\"Ground truth CSV must contain 'image_name' or 'image'.\")\n",
    "\n",
    "pred_annot_col = None\n",
    "for c in [\"annotations\", \"tooth_annotations\"]:\n",
    "    if c in pred_raw.columns:\n",
    "        pred_annot_col = c\n",
    "        break\n",
    "if pred_annot_col is None:\n",
    "    raise ValueError(\"Results CSV must contain 'annotations' or 'tooth_annotations'.\")\n",
    "\n",
    "pred_image_col = \"image_name\" if \"image_name\" in pred_raw.columns else (\"image\" if \"image\" in pred_raw.columns else None)\n",
    "if pred_image_col is None:\n",
    "    raise ValueError(\"Results CSV must contain 'image_name' or 'image'.\")\n",
    "\n",
    "\n",
    "\n",
    "gt_rows = []\n",
    "for _, r in gt_raw.iterrows():\n",
    "    img = r[gt_image_col]\n",
    "    for tooth_num, payload in extract_tooth_entries(r[\"tooth_annotations\"]):\n",
    "        row = {\"image_name\": img, \"tooth_num\": tooth_num}\n",
    "        for k in KEYS:\n",
    "            row[k] = normalise_none_token(payload.get(k, None))\n",
    "        gt_rows.append(row)\n",
    "\n",
    "gt_long = pd.DataFrame(gt_rows)\n",
    "if gt_long.empty:\n",
    "    raise ValueError(\"Parsed GT is empty. Check 'tooth_annotations' format.\")\n",
    "gt_long = gt_long.drop_duplicates(subset=[\"image_name\", \"tooth_num\"], keep=\"first\")\n",
    "\n",
    "\n",
    "\n",
    "pred_rows = []\n",
    "for _, r in pred_raw.iterrows():\n",
    "    img = r[pred_image_col]\n",
    "    for tooth_num, payload in extract_tooth_entries(r[pred_annot_col]):\n",
    "        out = {\"image_name\": img, \"tooth_num\": tooth_num}\n",
    "        for k in KEYS:\n",
    "            out[k] = normalise_none_token(payload.get(k, None))\n",
    "        pred_rows.append(out)\n",
    "\n",
    "pred_long = pd.DataFrame(pred_rows)\n",
    "if pred_long.empty:\n",
    "    raise ValueError(\"Parsed results is empty. Check predictions annotation column format.\")\n",
    "\n",
    "\n",
    "\n",
    "df = gt_long.merge(\n",
    "    pred_long,\n",
    "    on=[\"image_name\", \"tooth_num\"],\n",
    "    suffixes=(\"_gt\", \"_pred\"),\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "summary_rows = []\n",
    "\n",
    "for key in KEYS:\n",
    "    gt_col = f\"{key}_gt\"\n",
    "    pred_col = f\"{key}_pred\"\n",
    "\n",
    "    y_true, y_pred = [], []\n",
    "    for gt_val, pred_val in zip(df[gt_col].tolist(), df[pred_col].tolist()):\n",
    "        ok, gt_v, pred_v = should_count_pair(gt_val, pred_val, key)\n",
    "        if not ok:\n",
    "            continue\n",
    "        y_true.append(gt_v)\n",
    "        y_pred.append(pred_v)\n",
    "\n",
    "    n_used = len(y_true)\n",
    "    if n_used == 0:\n",
    "        with open(os.path.join(OUT_DIR, f\"{key}_NO_COUNTS.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\n",
    "                \"No valid pairs to count under rules.\\n\"\n",
    "                \"Mesial/Distal: None treated as Healthy.\\n\"\n",
    "                \"ARR+PLS: None treated as False (not skipped).\\n\"\n",
    "                \"Furcation: GT None skipped; missing pred treated as False.\\n\"\n",
    "                \"Keys containing '_conf' are ignored.\\n\"\n",
    "            )\n",
    "        summary_rows.append({\"key\": key, \"n_used\": 0})\n",
    "        continue\n",
    "\n",
    "    if key in MESIAL_DISTAL_KEYS:\n",
    "        labels = MESIAL_DISTAL_CLASSES\n",
    "        extra = sorted((set(y_true) | set(y_pred)) - set(labels))\n",
    "        labels = labels + extra\n",
    "    elif key in ARR_KEYS or key in PLS_KEYS or key in FURCATION_KEYS:\n",
    "        labels = BINARY_CLASSES\n",
    "        extra = sorted((set(y_true) | set(y_pred)) - set(labels), key=lambda x: str(x))\n",
    "        labels = labels + extra\n",
    "    else:\n",
    "        labels = sorted(set(y_true) | set(y_pred), key=lambda x: str(x))\n",
    "\n",
    "    cm_df = make_confusion_df(y_true, y_pred, labels=labels)\n",
    "    cm_df.to_csv(os.path.join(OUT_DIR, f\"{key}_confusion_matrix.csv\"), index=True)\n",
    "\n",
    "    if key in MESIAL_DISTAL_KEYS:\n",
    "        y_true_bin = [to_binary_md(v) for v in y_true]\n",
    "        y_pred_bin = [to_binary_md(v) for v in y_pred]\n",
    "        cm_bin_df = make_confusion_df(y_true_bin, y_pred_bin, labels=BINARY_CLASSES)\n",
    "        cm_bin_df.to_csv(os.path.join(OUT_DIR, f\"{key}_binary_confusion_matrix.csv\"), index=True)\n",
    "\n",
    "    summary_rows.append({\n",
    "        \"key\": key,\n",
    "        \"n_used\": n_used,\n",
    "        \"labels\": \"|\".join(map(str, labels)),\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows).sort_values([\"key\"])\n",
    "summary_df.to_csv(os.path.join(OUT_DIR, \"SUMMARY_counts_and_labels.csv\"), index=False)\n",
    "\n",
    "print(f\"Done. Wrote confusion matrices to: {OUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7856a0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped files (not part of configured tasks):\n",
      "  - distal_confusion_matrix.csv\n",
      "  - mesial_confusion_matrix.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Banksy\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\lib\\_nanfunctions_impl.py:1409: RuntimeWarning: All-NaN slice encountered\n",
      "  return _nanquantile_unchecked(\n",
      "C:\\Users\\Banksy\\AppData\\Local\\Temp\\ipykernel_13392\\3757361090.py:258: FutureWarning: The default value of observed=False is deprecated and will change to observed=True in a future version of pandas. Specify observed=False to silence this warning and retain the current behavior\n",
      "  pivot = df.pivot_table(index=\"dentist_id\", columns=\"group\", values=metric, aggfunc=\"first\")\n",
      "C:\\Users\\Banksy\\AppData\\Local\\Temp\\ipykernel_13392\\3757361090.py:258: FutureWarning: The default value of observed=False is deprecated and will change to observed=True in a future version of pandas. Specify observed=False to silence this warning and retain the current behavior\n",
      "  pivot = df.pivot_table(index=\"dentist_id\", columns=\"group\", values=metric, aggfunc=\"first\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Outputs written to:\n",
      "  F:\\Github Repos\\study\\study_code\\processed_data\\object_detection\\results_dentists\n",
      "Plots written to:\n",
      "  F:\\Github Repos\\study\\study_code\\processed_data\\object_detection\\results_dentists\\plots\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "root = Path(\"F:/Github Repos/study/study_code/processed_data/object_detection/confusion_matrices\")\n",
    "out_dir = Path(\"F:/Github Repos/study/study_code/processed_data/object_detection/results_dentists\")\n",
    "n_boot = 2000\n",
    "seed = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _normalise_label(s: str) -> str:\n",
    "    s = str(s)\n",
    "    if s.startswith(\"GT_\"):\n",
    "        return s[3:]\n",
    "    if s.startswith(\"PRED_\"):\n",
    "        return s[5:]\n",
    "    return s\n",
    "\n",
    "\n",
    "def load_confusion_csv(csv_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a confusion matrix CSV with a first column as GT labels (often 'Unnamed: 0'),\n",
    "    and columns for predicted labels.\n",
    "    Returns a DataFrame with index=GT labels, columns=PRED labels, values=int counts.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    gt_col = df.columns[0]\n",
    "    df = df.rename(columns={gt_col: \"GT_LABEL\"})\n",
    "    df[\"GT_LABEL\"] = df[\"GT_LABEL\"].map(_normalise_label)\n",
    "\n",
    "    pred_cols = [c for c in df.columns if c != \"GT_LABEL\"]\n",
    "    renamed = {c: _normalise_label(c) for c in pred_cols}\n",
    "    df = df.rename(columns=renamed)\n",
    "\n",
    "    df = df.set_index(\"GT_LABEL\")\n",
    "    df = df.apply(pd.to_numeric, errors=\"raise\").astype(int)\n",
    "\n",
    "    if (df.values < 0).any():\n",
    "        raise ValueError(f\"Negative counts found in {csv_path}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BinaryCounts:\n",
    "    tn: int\n",
    "    fp: int\n",
    "    fn: int\n",
    "    tp: int\n",
    "\n",
    "\n",
    "def _safe_div(num: float, den: float) -> float:\n",
    "    return float(num) / float(den) if den != 0 else np.nan\n",
    "\n",
    "\n",
    "def binary_counts_from_cm(\n",
    "    cm: pd.DataFrame,\n",
    "    negative_label: str = \"False\",\n",
    "    positive_label: str = \"True\",\n",
    ") -> BinaryCounts:\n",
    "    \"\"\"\n",
    "    Extract TN/FP/FN/TP from a 2x2 confusion matrix with GT rows and PRED cols.\n",
    "    \"\"\"\n",
    "    idx = {str(i): i for i in cm.index}\n",
    "    col = {str(c): c for c in cm.columns}\n",
    "\n",
    "    def pick(mapping: Dict[str, str], target: str) -> str:\n",
    "        if target in mapping:\n",
    "            return mapping[target]\n",
    "        for k, v in mapping.items():\n",
    "            if k.lower() == target.lower():\n",
    "                return v\n",
    "        raise KeyError(f\"Could not find label '{target}' in {list(mapping.keys())}\")\n",
    "\n",
    "    neg_i = pick(idx, negative_label)\n",
    "    pos_i = pick(idx, positive_label)\n",
    "    neg_c = pick(col, negative_label)\n",
    "    pos_c = pick(col, positive_label)\n",
    "\n",
    "    tn = int(cm.loc[neg_i, neg_c])\n",
    "    fp = int(cm.loc[neg_i, pos_c])\n",
    "    fn = int(cm.loc[pos_i, neg_c])\n",
    "    tp = int(cm.loc[pos_i, pos_c])\n",
    "    return BinaryCounts(tn=tn, fp=fp, fn=fn, tp=tp)\n",
    "\n",
    "\n",
    "def metrics_from_binary_counts(b: BinaryCounts) -> Dict[str, float]:\n",
    "    tn, fp, fn, tp = b.tn, b.fp, b.fn, b.tp\n",
    "    n = tn + fp + fn + tp\n",
    "\n",
    "    sens = _safe_div(tp, tp + fn)\n",
    "    spec = _safe_div(tn, tn + fp)\n",
    "    ppv = _safe_div(tp, tp + fp)\n",
    "    npv = _safe_div(tn, tn + fn)\n",
    "    f1 = _safe_div(2 * tp, 2 * tp + fp + fn)\n",
    "    acc = _safe_div(tp + tn, n)\n",
    "    bal_acc = np.nanmean([sens, spec])\n",
    "\n",
    "    mcc_num = (tp * tn) - (fp * fn)\n",
    "    mcc_den = np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "    mcc = _safe_div(mcc_num, mcc_den)\n",
    "\n",
    "    fnr = 1.0 - sens\n",
    "    fpr = 1.0 - spec\n",
    "    misdx = 1.0 - acc\n",
    "\n",
    "    return {\n",
    "        \"n\": float(n),\n",
    "        \"tp\": float(tp), \"tn\": float(tn), \"fp\": float(fp), \"fn\": float(fn),\n",
    "        \"sensitivity\": sens,\n",
    "        \"specificity\": spec,\n",
    "        \"precision_ppv\": ppv,\n",
    "        \"npv\": npv,\n",
    "        \"f1\": f1,\n",
    "        \"accuracy\": acc,\n",
    "        \"balanced_accuracy\": bal_acc,\n",
    "        \"mcc\": mcc,\n",
    "        \"false_negative_rate\": fnr,\n",
    "        \"false_positive_rate\": fpr,\n",
    "        \"misdiagnosis_rate\": misdx,\n",
    "    }\n",
    "\n",
    "\n",
    "def metrics_multiclass(cm: pd.DataFrame) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Multiclass metrics with one-vs-rest macro precision/recall/F1 AND macro-specificity.\n",
    "    Also returns:\n",
    "      false_negative_rate = 1 - macro_recall\n",
    "      false_positive_rate = 1 - macro_specificity\n",
    "    \"\"\"\n",
    "    cmv = cm.values.astype(float)\n",
    "    n = cmv.sum()\n",
    "    acc = _safe_div(np.trace(cmv), n)\n",
    "\n",
    "    tp = np.diag(cmv)\n",
    "    pred_sum = cmv.sum(axis=0)\n",
    "    gt_sum = cmv.sum(axis=1)\n",
    "\n",
    "    fp = pred_sum - tp\n",
    "    fn = gt_sum - tp\n",
    "    tn = n - tp - fp - fn\n",
    "\n",
    "    prec = np.array([_safe_div(tp[i], pred_sum[i]) for i in range(len(tp))], dtype=float)\n",
    "    rec  = np.array([_safe_div(tp[i], gt_sum[i]) for i in range(len(tp))], dtype=float)\n",
    "    f1   = np.array([_safe_div(2 * prec[i] * rec[i], prec[i] + rec[i]) for i in range(len(tp))], dtype=float)\n",
    "    spec = np.array([_safe_div(tn[i], tn[i] + fp[i]) for i in range(len(tp))], dtype=float)\n",
    "\n",
    "    macro_prec = np.nanmean(prec)\n",
    "    macro_rec  = np.nanmean(rec)\n",
    "    macro_f1   = np.nanmean(f1)\n",
    "    macro_spec = np.nanmean(spec)\n",
    "\n",
    "    fnr = 1.0 - macro_rec\n",
    "    fpr = 1.0 - macro_spec\n",
    "\n",
    "    return {\n",
    "        \"n\": float(n),\n",
    "        \"accuracy\": acc,\n",
    "        \"macro_precision\": macro_prec,\n",
    "        \"macro_recall\": macro_rec,\n",
    "        \"macro_f1\": macro_f1,\n",
    "        \"balanced_accuracy\": macro_rec,\n",
    "        \"macro_specificity\": macro_spec,\n",
    "        \"false_negative_rate\": fnr,\n",
    "        \"false_positive_rate\": fpr,\n",
    "    }\n",
    "\n",
    "\n",
    "def bootstrap_binary_cis(b: BinaryCounts, n_boot: int = 2000, seed: int = 0) -> Dict[str, Tuple[float, float]]:\n",
    "    \"\"\"\n",
    "    Bootstrap CIs using multinomial resampling over the 4 cells.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    counts = np.array([b.tn, b.fp, b.fn, b.tp], dtype=int)\n",
    "    n = counts.sum()\n",
    "    if n == 0:\n",
    "        return {}\n",
    "\n",
    "    p = counts / n\n",
    "    boots = []\n",
    "    for _ in range(n_boot):\n",
    "        sample = rng.multinomial(n, p)\n",
    "        bb = BinaryCounts(tn=int(sample[0]), fp=int(sample[1]), fn=int(sample[2]), tp=int(sample[3]))\n",
    "        m = metrics_from_binary_counts(bb)\n",
    "        boots.append([\n",
    "            m[\"false_negative_rate\"],\n",
    "            m[\"false_positive_rate\"],\n",
    "            m[\"sensitivity\"],\n",
    "            m[\"specificity\"],\n",
    "            m[\"balanced_accuracy\"],\n",
    "            m[\"f1\"],\n",
    "            m[\"mcc\"],\n",
    "        ])\n",
    "\n",
    "    arr = np.asarray(boots, dtype=float)\n",
    "    keys = [\"false_negative_rate\", \"false_positive_rate\", \"sensitivity\", \"specificity\", \"balanced_accuracy\", \"f1\", \"mcc\"]\n",
    "    cis = {}\n",
    "    for i, k in enumerate(keys):\n",
    "        lo, hi = np.nanpercentile(arr[:, i], [2.5, 97.5])\n",
    "        cis[k] = (float(lo), float(hi))\n",
    "    return cis\n",
    "\n",
    "\n",
    "def infer_task_name(csv_path: Path) -> str:\n",
    "    name = csv_path.stem\n",
    "    for suffix in [\"_confusion_matrix\", \"confusion_matrix\"]:\n",
    "        if name.endswith(suffix):\n",
    "            name = name[: -len(suffix)]\n",
    "            break\n",
    "    return name.strip(\"_\")\n",
    "\n",
    "\n",
    "def aggregate_confusions(confusions: List[pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Sum confusion matrices with potentially different label sets.\n",
    "    \"\"\"\n",
    "    if not confusions:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    all_gt = sorted(set().union(*[set(c.index) for c in confusions]))\n",
    "    all_pr = sorted(set().union(*[set(c.columns) for c in confusions]))\n",
    "\n",
    "    agg = pd.DataFrame(0, index=all_gt, columns=all_pr, dtype=int)\n",
    "    for c in confusions:\n",
    "        agg.loc[c.index, c.columns] += c.astype(int)\n",
    "    return agg\n",
    "\n",
    "\n",
    "def plot_metric_heatmap(per_group_dentist: pd.DataFrame,\n",
    "                        metric: str,\n",
    "                        out_path: Path,\n",
    "                        title: str,\n",
    "                        group_order: List[str],\n",
    "                        group_labels: List[str],\n",
    "                        value_fmt: str = \"{:.3f}\") -> None:\n",
    "    \"\"\"\n",
    "    Heatmap with per-cell numeric annotations.\n",
    "    Rows: dentists (D1, D2, ...)\n",
    "    Cols: groups (PBL, ARR, PLS, Furcation)\n",
    "    \"\"\"\n",
    "    df = per_group_dentist.copy()\n",
    "    df = df[df[\"group\"].isin(group_order)].copy()\n",
    "    df[\"group\"] = pd.Categorical(df[\"group\"], categories=group_order, ordered=True)\n",
    "\n",
    "    pivot = df.pivot_table(index=\"dentist_id\", columns=\"group\", values=metric, aggfunc=\"first\")\n",
    "    pivot = pivot.reindex(columns=group_order).sort_index()\n",
    "\n",
    "    data = pivot.values.astype(float)\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 6), constrained_layout=True)\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    im = ax.imshow(data, aspect=\"auto\")\n",
    "\n",
    "    ax.set_yticks(np.arange(pivot.shape[0]))\n",
    "    ax.set_yticklabels(pivot.index)\n",
    "\n",
    "    ax.set_xticks(np.arange(pivot.shape[1]))\n",
    "    ax.set_xticklabels(group_labels, rotation=0)\n",
    "\n",
    "    ax.set_xlabel(\"Task\", labelpad=10)\n",
    "    ax.set_ylabel(\"Dentist\", labelpad=10)\n",
    "    ax.set_title(title)\n",
    "\n",
    "    fig.colorbar(im, ax=ax)\n",
    "\n",
    "    for i in range(pivot.shape[0]):\n",
    "        for j in range(pivot.shape[1]):\n",
    "            v = data[i, j]\n",
    "            txt = \"NA\" if np.isnan(v) else value_fmt.format(v)\n",
    "            ax.text(j, i, txt, ha=\"center\", va=\"center\")\n",
    "\n",
    "    fig.savefig(out_path, dpi=200)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "plots_dir = out_dir / \"plots\"\n",
    "plots_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "GROUPS = {\n",
    "    \"mesial\":    {\"tasks\": [\"mesial_binary\"],                  \"type\": \"binary\"},\n",
    "    \"distal\":    {\"tasks\": [\"distal_binary\"],                  \"type\": \"binary\"},\n",
    "    \"PBL\":       {\"tasks\": [\"mesial_binary\", \"distal_binary\"], \"type\": \"binary\"},\n",
    "    \"ARR\":       {\"tasks\": [\"arr_left\", \"arr_right\"],          \"type\": \"binary\"},\n",
    "    \"Furcation\": {\"tasks\": [\"furcation\"],                      \"type\": \"binary\"},\n",
    "    \"PLS\":       {\"tasks\": [\"mesial_pls\", \"distal_pls\"],       \"type\": \"binary\"},\n",
    "}\n",
    "\n",
    "KNOWN_TASKS = sorted({t for spec in GROUPS.values() for t in spec[\"tasks\"]}, key=len, reverse=True)\n",
    "\n",
    "\n",
    "def collect_confusion_files_flat(folder: Path, pattern: str = \"*confusion_matrix.csv\") -> List[Path]:\n",
    "    return sorted(folder.glob(pattern))\n",
    "\n",
    "\n",
    "def infer_dentist_and_task(csv_path: Path, known_tasks: List[str]) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    We infer task by matching the filename (minus confusion_matrix suffix) to a known task.\n",
    "    Dentist is whatever precedes the task, or 'ALL' if none.\n",
    "    Examples:\n",
    "      D1_mesial_binary_confusion_matrix.csv -> dentist_raw=D1, task=mesial_binary\n",
    "      mesial_binary_confusion_matrix.csv    -> dentist_raw=ALL, task=mesial_binary\n",
    "    \"\"\"\n",
    "    base = infer_task_name(csv_path)\n",
    "\n",
    "    for t in known_tasks:\n",
    "        if base == t:\n",
    "            return \"ALL\", t\n",
    "        for sep in [\"__\", \"_\", \"-\"]:\n",
    "            suf = f\"{sep}{t}\"\n",
    "            if base.endswith(suf):\n",
    "                dentist_raw = base[: -len(suf)].strip(\"_-\")\n",
    "                return (dentist_raw if dentist_raw else \"ALL\"), t\n",
    "\n",
    "    raise ValueError(\n",
    "        f\"Could not infer task from filename '{csv_path.name}'. Base='{base}'. \"\n",
    "        f\"Expected it to end with one of known tasks: {known_tasks}\"\n",
    "    )\n",
    "\n",
    "\n",
    "all_rows = []\n",
    "csv_files = collect_confusion_files_flat(root)\n",
    "\n",
    "if not csv_files:\n",
    "    raise RuntimeError(f\"No confusion_matrix.csv files found directly under: {root}\")\n",
    "\n",
    "skipped = []\n",
    "dentist_raws_seen = set()\n",
    "\n",
    "for csv_path in csv_files:\n",
    "    try:\n",
    "        dentist_raw, task = infer_dentist_and_task(csv_path, KNOWN_TASKS)\n",
    "    except ValueError:\n",
    "        skipped.append(csv_path.name)\n",
    "        continue\n",
    "\n",
    "    cm = load_confusion_csv(csv_path)\n",
    "\n",
    "    dentist_raws_seen.add(dentist_raw)\n",
    "    all_rows.append({\n",
    "        \"dentist_raw\": dentist_raw,\n",
    "        \"task\": task,\n",
    "        \"file\": str(csv_path),\n",
    "        \"cm\": cm,\n",
    "    })\n",
    "\n",
    "all_df = pd.DataFrame(all_rows)\n",
    "if all_df.empty:\n",
    "    raise RuntimeError(\n",
    "        f\"No recognised confusion matrix files found under: {root}\\n\"\n",
    "        f\"Skipped: {skipped}\"\n",
    "    )\n",
    "\n",
    "\n",
    "dentist_raws = sorted(dentist_raws_seen)\n",
    "dentist_map = {raw: f\"D{i+1}\" for i, raw in enumerate(dentist_raws)}\n",
    "all_df[\"dentist_id\"] = all_df[\"dentist_raw\"].map(dentist_map)\n",
    "\n",
    "if skipped:\n",
    "    print(\"Skipped files (not part of configured tasks):\")\n",
    "    for s in skipped:\n",
    "        print(f\"  - {s}\")\n",
    "\n",
    "\n",
    "per_group_file_rows = []\n",
    "per_group_dentist_rows = []\n",
    "\n",
    "for group_name, spec in GROUPS.items():\n",
    "    tasks = set(spec[\"tasks\"])\n",
    "    group_type = spec[\"type\"]\n",
    "\n",
    "    group_out = out_dir / group_name\n",
    "    group_out.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    gdf = all_df[all_df[\"task\"].isin(tasks)].copy()\n",
    "\n",
    "    for _, r in gdf.iterrows():\n",
    "        cm = r[\"cm\"]\n",
    "\n",
    "        if group_type == \"binary\":\n",
    "            if cm.shape != (2, 2):\n",
    "                raise ValueError(f\"[{group_name}] Expected binary 2x2 but got {cm.shape} in {r['file']}\")\n",
    "            b = binary_counts_from_cm(cm, \"False\", \"True\")\n",
    "            m = metrics_from_binary_counts(b)\n",
    "        else:\n",
    "            m = metrics_multiclass(cm)\n",
    "\n",
    "        per_group_file_rows.append({\n",
    "            \"group\": group_name,\n",
    "            \"dentist_raw\": r[\"dentist_raw\"],\n",
    "            \"dentist_id\": r[\"dentist_id\"],\n",
    "            \"task\": r[\"task\"],\n",
    "            \"file\": r[\"file\"],\n",
    "            **m\n",
    "        })\n",
    "\n",
    "    for dentist_raw, sub in gdf.groupby(\"dentist_raw\"):\n",
    "        dentist_id = dentist_map[dentist_raw]\n",
    "        cms = list(sub[\"cm\"].values)\n",
    "        agg = aggregate_confusions(cms)\n",
    "\n",
    "        if agg.empty:\n",
    "            continue\n",
    "\n",
    "        if group_type == \"binary\":\n",
    "            if agg.shape != (2, 2):\n",
    "                raise ValueError(f\"[{group_name}] Expected aggregated binary 2x2 but got {agg.shape} for {dentist_raw}\")\n",
    "            b = binary_counts_from_cm(agg, \"False\", \"True\")\n",
    "            m = metrics_from_binary_counts(b)\n",
    "            cis = bootstrap_binary_cis(b, n_boot=n_boot, seed=seed)\n",
    "\n",
    "            row = {\"group\": group_name, \"dentist_raw\": dentist_raw, \"dentist_id\": dentist_id, **m}\n",
    "            for k, (lo, hi) in cis.items():\n",
    "                row[f\"{k}_ci_low\"] = lo\n",
    "                row[f\"{k}_ci_high\"] = hi\n",
    "            per_group_dentist_rows.append(row)\n",
    "        else:\n",
    "            m = metrics_multiclass(agg)\n",
    "            per_group_dentist_rows.append({\"group\": group_name, \"dentist_raw\": dentist_raw, \"dentist_id\": dentist_id, **m})\n",
    "\n",
    "    per_file_g = pd.DataFrame([r for r in per_group_file_rows if r[\"group\"] == group_name])\n",
    "    per_dent_g = pd.DataFrame([r for r in per_group_dentist_rows if r[\"group\"] == group_name])\n",
    "\n",
    "    per_file_g.to_csv(group_out / f\"per_file_metrics_{group_name}.csv\", index=False)\n",
    "    per_dent_g.to_csv(group_out / f\"per_dentist_metrics_{group_name}.csv\", index=False)\n",
    "\n",
    "# Save global combined tables\n",
    "per_file = pd.DataFrame(per_group_file_rows)\n",
    "per_dentist = pd.DataFrame(per_group_dentist_rows)\n",
    "per_file.to_csv(out_dir / \"per_file_metrics_ALLGROUPS.csv\", index=False)\n",
    "per_dentist.to_csv(out_dir / \"per_dentist_metrics_ALLGROUPS.csv\", index=False)\n",
    "\n",
    "# Plots\n",
    "plot_groups = [\"PBL\", \"ARR\", \"PLS\", \"Furcation\"]\n",
    "plot_labels = [\"PBL (mesial+distal, binary)\", \"ARR\", \"PLS\", \"Furcation\"]\n",
    "\n",
    "plot_df = per_dentist[per_dentist[\"group\"].isin(plot_groups)].copy()\n",
    "\n",
    "plot_metric_heatmap(\n",
    "    per_group_dentist=plot_df,\n",
    "    metric=\"false_negative_rate\",\n",
    "    out_path=plots_dir / \"heatmap_false_negative_rate.png\",\n",
    "    title=\"False Negative Rate (1 - recall)\",\n",
    "    group_order=plot_groups,\n",
    "    group_labels=plot_labels,\n",
    ")\n",
    "\n",
    "plot_metric_heatmap(\n",
    "    per_group_dentist=plot_df,\n",
    "    metric=\"false_positive_rate\",\n",
    "    out_path=plots_dir / \"heatmap_false_positive_rate.png\",\n",
    "    title=\"False Positive Rate (1 - specificity)\",\n",
    "    group_order=plot_groups,\n",
    "    group_labels=plot_labels,\n",
    ")\n",
    "\n",
    "print(\"Done. Outputs written to:\")\n",
    "print(f\"  {out_dir}\")\n",
    "print(\"Plots written to:\")\n",
    "print(f\"  {plots_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa16fdac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}[!htbp]\n",
      "\\centering\n",
      "\\scriptsize\n",
      "\\begin{adjustbox}{center, max width=\\paperwidth}\n",
      "\\begin{tabular}{l|cccccc}\n",
      "\\toprule\n",
      "\\multicolumn{7}{c}{Single-system Evaluation}\\\\\n",
      "\\midrule\n",
      "Evaluation & Precision & Recall & F1 & Specificity & FNR & FPR \\\\\n",
      "\\midrule\n",
      "Mesial (binary) & 1.000 & 0.667 & 0.800 & 1.000 & 0.333 & 0.000 \\\\\n",
      "\\midrule\n",
      "Distal (binary) & 0.889 & 0.400 & 0.552 & 0.889 & 0.600 & 0.111 \\\\\n",
      "\\midrule\n",
      "PBL (mesial+distal, binary) & 0.280 & 0.299 & 0.299 & 0.782 & 0.701 & 0.218 \\\\\n",
      "\\midrule\n",
      "ARR & 1.000 & 0.375 & 0.545 & 1.000 & 0.625 & 0.000 \\\\\n",
      "\\midrule\n",
      "PLS & 0.600 & 0.600 & 0.600 & 0.962 & 0.400 & 0.038 \\\\\n",
      "\\midrule\n",
      "Furcation & 0.286 & 1.000 & 0.444 & 0.444 & 0.000 & 0.556 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{adjustbox}\n",
      "\\caption{Single-system results across evaluation types. Metrics are computed from aggregated confusion matrices per evaluation type. For binary tasks, macro metrics are set to their binary equivalents.}\n",
      "\\label{tab:single_system_eval}\n",
      "\\end{table}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Banksy\\AppData\\Local\\Temp\\ipykernel_13392\\2962493616.py:146: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  .groupby(\"group\", as_index=False)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "out_dir = Path(r\"F:/Github Repos/study/study_code/processed_data/keypoint/results_dentists_multiclass\")\n",
    "# results_path = out_dir / \"per_task_metrics.csv\"\n",
    "results_path = out_dir / \"per_dentist_metrics_ALLGROUPS.csv\"\n",
    "\n",
    "\n",
    "\n",
    "GROUP_ORDER = [\"mesial\", \"distal\", \"PBL\", \"ARR\", \"PLS\", \"Furcation\"]\n",
    "GROUP_LABELS = {\n",
    "    \"mesial\": \"Mesial (binary)\",\n",
    "    \"distal\": \"Distal (binary)\",\n",
    "    \"PBL\": \"PBL (mesial+distal, binary)\",\n",
    "    \"ARR\": \"ARR\",\n",
    "    \"PLS\": \"PLS\",\n",
    "    \"Furcation\": \"Furcation\",\n",
    "}\n",
    "\n",
    "\n",
    "METRICS = [\n",
    "    (\"macro_precision\", \"Precision\"),\n",
    "    (\"macro_recall\", \"Recall\"),\n",
    "    (\"macro_f1\", \"F1\"),\n",
    "    (\"macro_specificity\", \"Specificity\"),\n",
    "    (\"false_negative_rate\", \"FNR\"),\n",
    "    (\"false_positive_rate\", \"FPR\"),\n",
    "]\n",
    "\n",
    "CAPTION = (\n",
    "    \"Single-system results across evaluation types. Metrics are computed from aggregated confusion matrices \"\n",
    "    \"per evaluation type. For binary tasks, macro metrics are set to their binary equivalents.\"\n",
    ")\n",
    "LABEL = \"tab:single_system_eval\"\n",
    "\n",
    "\n",
    "def _fmt(x: float, ndp: int = 3) -> str:\n",
    "    if x is None or (isinstance(x, float) and np.isnan(x)):\n",
    "        return \"NA\"\n",
    "    return f\"{float(x):.{ndp}f}\"\n",
    "\n",
    "\n",
    "def _latex_escape(s: str) -> str:\n",
    "    s = str(s)\n",
    "    return (s.replace(\"\\\\\", r\"\\textbackslash{}\")\n",
    "             .replace(\"&\", r\"\\&\")\n",
    "             .replace(\"%\", r\"\\%\")\n",
    "             .replace(\"$\", r\"\\$\")\n",
    "             .replace(\"#\", r\"\\#\")\n",
    "             .replace(\"_\", r\"\\_\")\n",
    "             .replace(\"{\", r\"\\{\")\n",
    "             .replace(\"}\", r\"\\}\")\n",
    "             .replace(\"~\", r\"\\textasciitilde{}\")\n",
    "             .replace(\"^\", r\"\\textasciicircum{}\"))\n",
    "\n",
    "\n",
    "def _is_binary_group(g: str) -> bool:\n",
    "    return g in {\"mesial\", \"distal\", \"PBL\", \"ARR\", \"PLS\", \"Furcation\"}\n",
    "\n",
    "\n",
    "def _autofind_results_csv(root: Path) -> Path:\n",
    "    cands = list(root.rglob(\"per_group_metrics.csv\"))\n",
    "    if cands:\n",
    "        return cands[0]\n",
    "    cands = list(root.rglob(\"*per_group_metrics*.csv\"))\n",
    "    if cands:\n",
    "        return cands[0]\n",
    "    raise FileNotFoundError(\n",
    "        f\"Could not find per_group_metrics.csv under: {root}\\n\"\n",
    "        \"Set `results_path` to the correct CSV.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def _ensure_macro_columns_for_binary_single(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For binary groups, map:\n",
    "      macro_precision    <- precision_ppv\n",
    "      macro_recall       <- sensitivity\n",
    "      macro_f1           <- f1\n",
    "      macro_specificity  <- specificity\n",
    "\n",
    "    Ensure FNR/FPR exist:\n",
    "      FNR <- 1 - macro_recall\n",
    "      FPR <- 1 - macro_specificity\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    is_bin = df[\"group\"].apply(_is_binary_group)\n",
    "\n",
    "    for c in [\"macro_precision\", \"macro_recall\", \"macro_f1\", \"macro_specificity\"]:\n",
    "        if c not in df.columns:\n",
    "            df[c] = np.nan\n",
    "\n",
    "    def fill(target: str, source: str):\n",
    "        if source in df.columns:\n",
    "            mask = is_bin & df[target].isna()\n",
    "            df.loc[mask, target] = pd.to_numeric(df.loc[mask, source], errors=\"coerce\")\n",
    "\n",
    "    fill(\"macro_precision\", \"precision_ppv\")\n",
    "    fill(\"macro_recall\", \"sensitivity\")\n",
    "    fill(\"macro_f1\", \"f1\")\n",
    "    fill(\"macro_specificity\", \"specificity\")\n",
    "\n",
    "    if \"false_negative_rate\" not in df.columns:\n",
    "        df[\"false_negative_rate\"] = 1.0 - pd.to_numeric(df[\"macro_recall\"], errors=\"coerce\")\n",
    "    if \"false_positive_rate\" not in df.columns:\n",
    "        df[\"false_positive_rate\"] = 1.0 - pd.to_numeric(df[\"macro_specificity\"], errors=\"coerce\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "if not results_path.exists():\n",
    "    results_path = _autofind_results_csv(out_dir)\n",
    "\n",
    "df = pd.read_csv(results_path)\n",
    "\n",
    "required = {\"group\"}\n",
    "missing = required - set(df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing required columns in results CSV: {missing}\")\n",
    "\n",
    "df = df[df[\"group\"].isin(GROUP_ORDER)].copy()\n",
    "df[\"group\"] = pd.Categorical(df[\"group\"], categories=GROUP_ORDER, ordered=True)\n",
    "\n",
    "df = _ensure_macro_columns_for_binary_single(df)\n",
    "\n",
    "keep_cols = [\"group\"] + [c for c, _ in METRICS]\n",
    "for c in keep_cols:\n",
    "    if c not in df.columns:\n",
    "        df[c] = np.nan\n",
    "df = df[keep_cols].copy()\n",
    "\n",
    "df = (df.sort_values([\"group\"])\n",
    "        .groupby(\"group\", as_index=False)\n",
    "        .first())\n",
    "\n",
    "metric_disp = [disp for _, disp in METRICS]\n",
    "n_metrics = len(metric_disp)\n",
    "\n",
    "colspec = \"l|\" + \"\".join([\"c\"] * n_metrics)\n",
    "\n",
    "lines = []\n",
    "lines.append(r\"\\begin{table}[!htbp]\")\n",
    "lines.append(r\"\\centering\")\n",
    "lines.append(r\"\\scriptsize\")\n",
    "lines.append(r\"\\begin{adjustbox}{center, max width=\\paperwidth}\")\n",
    "lines.append(r\"\\begin{tabular}{\" + colspec + r\"}\")\n",
    "lines.append(r\"\\toprule\")\n",
    "lines.append(r\"\\multicolumn{\" + str(1 + n_metrics) + r\"}{c}{Single-system Evaluation}\\\\\")\n",
    "lines.append(r\"\\midrule\")\n",
    "\n",
    "# Header row\n",
    "hdr = [\"Evaluation\"] + metric_disp\n",
    "lines.append(\" & \".join(map(_latex_escape, hdr)) + r\" \\\\\")\n",
    "lines.append(r\"\\midrule\")\n",
    "\n",
    "# Body, one row per evaluation type\n",
    "for gi, g in enumerate(GROUP_ORDER):\n",
    "    sub = df[df[\"group\"] == g].copy()\n",
    "    eval_name = GROUP_LABELS.get(g, g)\n",
    "\n",
    "    if sub.empty:\n",
    "        vals = [\"NA\"] * n_metrics\n",
    "    else:\n",
    "        row = sub.iloc[0]\n",
    "        vals = []\n",
    "        for col, _disp in METRICS:\n",
    "            vals.append(_fmt(pd.to_numeric(row[col], errors=\"coerce\")))\n",
    "\n",
    "    lines.append(_latex_escape(eval_name) + \" & \" + \" & \".join(vals) + r\" \\\\\")\n",
    "\n",
    "    if gi == len(GROUP_ORDER) - 1:\n",
    "        lines.append(r\"\\bottomrule\")\n",
    "    else:\n",
    "        lines.append(r\"\\midrule\")\n",
    "\n",
    "lines.append(r\"\\end{tabular}\")\n",
    "lines.append(r\"\\end{adjustbox}\")\n",
    "lines.append(r\"\\caption{\" + CAPTION + r\"}\")\n",
    "lines.append(r\"\\label{\" + LABEL + r\"}\")\n",
    "lines.append(r\"\\end{table}\")\n",
    "\n",
    "print(\"\\n\".join(lines))\n",
    "\n",
    "# Required LaTeX packages:\n",
    "# \\usepackage{booktabs}\n",
    "# \\usepackage{adjustbox}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d83f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Banksy\\AppData\\Local\\Temp\\ipykernel_13392\\969252952.py:257: FutureWarning: The default value of observed=False is deprecated and will change to observed=True in a future version of pandas. Specify observed=False to silence this warning and retain the current behavior\n",
      "  pivot = df.pivot_table(index=\"dentist_id\", columns=\"group\", values=metric, aggfunc=\"first\")\n",
      "C:\\Users\\Banksy\\AppData\\Local\\Temp\\ipykernel_13392\\969252952.py:257: FutureWarning: The default value of observed=False is deprecated and will change to observed=True in a future version of pandas. Specify observed=False to silence this warning and retain the current behavior\n",
      "  pivot = df.pivot_table(index=\"dentist_id\", columns=\"group\", values=metric, aggfunc=\"first\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Outputs written to:\n",
      "  F:\\Github Repos\\study\\study_code\\processed_data\\keypoint\\results_dentists_multiclass\n",
      "Plots written to:\n",
      "  F:\\Github Repos\\study\\study_code\\processed_data\\keypoint\\results_dentists_multiclass\\plots\n"
     ]
    }
   ],
   "source": [
    "### MULTI CLASS PBL\n",
    "\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "root = Path(\"F:/Github Repos/study/study_code/processed_data/keypoint/confusion_matrices\")\n",
    "out_dir = Path(\"F:/Github Repos/study/study_code/processed_data/keypoint/results_dentists_multiclass\")\n",
    "n_boot = 2000\n",
    "seed = 0\n",
    "\n",
    "\n",
    "\n",
    "def _normalise_label(s: str) -> str:\n",
    "    s = str(s)\n",
    "    if s.startswith(\"GT_\"):\n",
    "        return s[3:]\n",
    "    if s.startswith(\"PRED_\"):\n",
    "        return s[5:]\n",
    "    return s\n",
    "\n",
    "\n",
    "def load_confusion_csv(csv_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a confusion matrix CSV with a first column as GT labels (often 'Unnamed: 0'),\n",
    "    and columns for predicted labels.\n",
    "    Returns a DataFrame with index=GT labels, columns=PRED labels, values=int counts.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    gt_col = df.columns[0]\n",
    "    df = df.rename(columns={gt_col: \"GT_LABEL\"})\n",
    "    df[\"GT_LABEL\"] = df[\"GT_LABEL\"].map(_normalise_label)\n",
    "\n",
    "    pred_cols = [c for c in df.columns if c != \"GT_LABEL\"]\n",
    "    renamed = {c: _normalise_label(c) for c in pred_cols}\n",
    "    df = df.rename(columns=renamed)\n",
    "\n",
    "    df = df.set_index(\"GT_LABEL\")\n",
    "    df = df.apply(pd.to_numeric, errors=\"raise\").astype(int)\n",
    "\n",
    "    if (df.values < 0).any():\n",
    "        raise ValueError(f\"Negative counts found in {csv_path}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class BinaryCounts:\n",
    "    tn: int\n",
    "    fp: int\n",
    "    fn: int\n",
    "    tp: int\n",
    "\n",
    "\n",
    "def _safe_div(num: float, den: float) -> float:\n",
    "    return float(num) / float(den) if den != 0 else np.nan\n",
    "\n",
    "\n",
    "def binary_counts_from_cm(\n",
    "    cm: pd.DataFrame,\n",
    "    negative_label: str = \"False\",\n",
    "    positive_label: str = \"True\",\n",
    ") -> BinaryCounts:\n",
    "    \"\"\"\n",
    "    Extract TN/FP/FN/TP from a 2x2 confusion matrix with GT rows and PRED cols.\n",
    "    \"\"\"\n",
    "    idx = {str(i): i for i in cm.index}\n",
    "    col = {str(c): c for c in cm.columns}\n",
    "\n",
    "    def pick(mapping: Dict[str, str], target: str) -> str:\n",
    "        if target in mapping:\n",
    "            return mapping[target]\n",
    "        for k, v in mapping.items():\n",
    "            if k.lower() == target.lower():\n",
    "                return v\n",
    "        raise KeyError(f\"Could not find label '{target}' in {list(mapping.keys())}\")\n",
    "\n",
    "    neg_i = pick(idx, negative_label)\n",
    "    pos_i = pick(idx, positive_label)\n",
    "    neg_c = pick(col, negative_label)\n",
    "    pos_c = pick(col, positive_label)\n",
    "\n",
    "    tn = int(cm.loc[neg_i, neg_c])\n",
    "    fp = int(cm.loc[neg_i, pos_c])\n",
    "    fn = int(cm.loc[pos_i, neg_c])\n",
    "    tp = int(cm.loc[pos_i, pos_c])\n",
    "    return BinaryCounts(tn=tn, fp=fp, fn=fn, tp=tp)\n",
    "\n",
    "\n",
    "def metrics_from_binary_counts(b: BinaryCounts) -> Dict[str, float]:\n",
    "    tn, fp, fn, tp = b.tn, b.fp, b.fn, b.tp\n",
    "    n = tn + fp + fn + tp\n",
    "\n",
    "    sens = _safe_div(tp, tp + fn)\n",
    "    spec = _safe_div(tn, tn + fp)\n",
    "    ppv = _safe_div(tp, tp + fp)\n",
    "    npv = _safe_div(tn, tn + fn)\n",
    "    f1 = _safe_div(2 * tp, 2 * tp + fp + fn)\n",
    "    acc = _safe_div(tp + tn, n)\n",
    "    bal_acc = np.nanmean([sens, spec])\n",
    "\n",
    "    mcc_num = (tp * tn) - (fp * fn)\n",
    "    mcc_den = np.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "    mcc = _safe_div(mcc_num, mcc_den)\n",
    "\n",
    "    fnr = 1.0 - sens\n",
    "    fpr = 1.0 - spec\n",
    "    misdx = 1.0 - acc\n",
    "\n",
    "    return {\n",
    "        \"n\": float(n),\n",
    "        \"tp\": float(tp), \"tn\": float(tn), \"fp\": float(fp), \"fn\": float(fn),\n",
    "        \"sensitivity\": sens,\n",
    "        \"specificity\": spec,\n",
    "        \"precision_ppv\": ppv,\n",
    "        \"npv\": npv,\n",
    "        \"f1\": f1,\n",
    "        \"accuracy\": acc,\n",
    "        \"balanced_accuracy\": bal_acc,\n",
    "        \"mcc\": mcc,\n",
    "        \"false_negative_rate\": fnr,\n",
    "        \"false_positive_rate\": fpr,\n",
    "        \"misdiagnosis_rate\": misdx,\n",
    "    }\n",
    "\n",
    "\n",
    "def metrics_multiclass(cm: pd.DataFrame) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Multiclass metrics with one-vs-rest macro precision/recall/F1 AND macro-specificity.\n",
    "    Also returns:\n",
    "      false_negative_rate = 1 - macro_recall\n",
    "      false_positive_rate = 1 - macro_specificity\n",
    "    \"\"\"\n",
    "    cmv = cm.values.astype(float)\n",
    "    n = cmv.sum()\n",
    "    acc = _safe_div(np.trace(cmv), n)\n",
    "\n",
    "    tp = np.diag(cmv)\n",
    "    pred_sum = cmv.sum(axis=0)\n",
    "    gt_sum = cmv.sum(axis=1)\n",
    "\n",
    "    fp = pred_sum - tp\n",
    "    fn = gt_sum - tp\n",
    "    tn = n - tp - fp - fn\n",
    "\n",
    "    prec = np.array([_safe_div(tp[i], pred_sum[i]) for i in range(len(tp))], dtype=float)\n",
    "    rec  = np.array([_safe_div(tp[i], gt_sum[i]) for i in range(len(tp))], dtype=float)\n",
    "    f1   = np.array([_safe_div(2 * prec[i] * rec[i], prec[i] + rec[i]) for i in range(len(tp))], dtype=float)\n",
    "    spec = np.array([_safe_div(tn[i], tn[i] + fp[i]) for i in range(len(tp))], dtype=float)\n",
    "\n",
    "    macro_prec = np.nanmean(prec)\n",
    "    macro_rec  = np.nanmean(rec)\n",
    "    macro_f1   = np.nanmean(f1)\n",
    "    macro_spec = np.nanmean(spec)\n",
    "\n",
    "    fnr = 1.0 - macro_rec\n",
    "    fpr = 1.0 - macro_spec\n",
    "\n",
    "    return {\n",
    "        \"n\": float(n),\n",
    "        \"accuracy\": acc,\n",
    "        \"macro_precision\": macro_prec,\n",
    "        \"macro_recall\": macro_rec,\n",
    "        \"macro_f1\": macro_f1,\n",
    "        \"balanced_accuracy\": macro_rec,\n",
    "        \"macro_specificity\": macro_spec,\n",
    "        \"false_negative_rate\": fnr,\n",
    "        \"false_positive_rate\": fpr,\n",
    "    }\n",
    "\n",
    "\n",
    "def bootstrap_binary_cis(b: BinaryCounts, n_boot: int = 2000, seed: int = 0) -> Dict[str, Tuple[float, float]]:\n",
    "    \"\"\"\n",
    "    Bootstrap CIs using multinomial resampling over the 4 cells.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    counts = np.array([b.tn, b.fp, b.fn, b.tp], dtype=int)\n",
    "    n = counts.sum()\n",
    "    if n == 0:\n",
    "        return {}\n",
    "\n",
    "    p = counts / n\n",
    "    boots = []\n",
    "    for _ in range(n_boot):\n",
    "        sample = rng.multinomial(n, p)\n",
    "        bb = BinaryCounts(tn=int(sample[0]), fp=int(sample[1]), fn=int(sample[2]), tp=int(sample[3]))\n",
    "        m = metrics_from_binary_counts(bb)\n",
    "        boots.append([\n",
    "            m[\"false_negative_rate\"],\n",
    "            m[\"false_positive_rate\"],\n",
    "            m[\"sensitivity\"],\n",
    "            m[\"specificity\"],\n",
    "            m[\"balanced_accuracy\"],\n",
    "            m[\"f1\"],\n",
    "            m[\"mcc\"],\n",
    "        ])\n",
    "\n",
    "    arr = np.asarray(boots, dtype=float)\n",
    "    keys = [\"false_negative_rate\", \"false_positive_rate\", \"sensitivity\", \"specificity\", \"balanced_accuracy\", \"f1\", \"mcc\"]\n",
    "    cis = {}\n",
    "    for i, k in enumerate(keys):\n",
    "        lo, hi = np.nanpercentile(arr[:, i], [2.5, 97.5])\n",
    "        cis[k] = (float(lo), float(hi))\n",
    "    return cis\n",
    "\n",
    "\n",
    "def infer_task_name(csv_path: Path) -> str:\n",
    "    name = csv_path.stem\n",
    "    for suffix in [\"_confusion_matrix\", \"confusion_matrix\"]:\n",
    "        if name.endswith(suffix):\n",
    "            name = name[: -len(suffix)]\n",
    "            break\n",
    "    return name.strip(\"_\")\n",
    "\n",
    "\n",
    "def aggregate_confusions(confusions: List[pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Sum confusion matrices with potentially different label sets.\n",
    "    \"\"\"\n",
    "    if not confusions:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    all_gt = sorted(set().union(*[set(c.index) for c in confusions]))\n",
    "    all_pr = sorted(set().union(*[set(c.columns) for c in confusions]))\n",
    "\n",
    "    agg = pd.DataFrame(0, index=all_gt, columns=all_pr, dtype=int)\n",
    "    for c in confusions:\n",
    "        agg.loc[c.index, c.columns] += c.astype(int)\n",
    "    return agg\n",
    "\n",
    "\n",
    "def plot_metric_heatmap(per_group_dentist: pd.DataFrame,\n",
    "                        metric: str,\n",
    "                        out_path: Path,\n",
    "                        title: str,\n",
    "                        group_order: List[str],\n",
    "                        group_labels: List[str],\n",
    "                        value_fmt: str = \"{:.3f}\") -> None:\n",
    "    \"\"\"\n",
    "    Heatmap with per-cell numeric annotations.\n",
    "    Rows: dentists (D1, D2, ...)\n",
    "    Cols: groups (PBL, ARR, PLS, Furcation)\n",
    "    \"\"\"\n",
    "    df = per_group_dentist.copy()\n",
    "    df = df[df[\"group\"].isin(group_order)].copy()\n",
    "    df[\"group\"] = pd.Categorical(df[\"group\"], categories=group_order, ordered=True)\n",
    "\n",
    "    pivot = df.pivot_table(index=\"dentist_id\", columns=\"group\", values=metric, aggfunc=\"first\")\n",
    "    pivot = pivot.reindex(columns=group_order).sort_index()\n",
    "\n",
    "    data = pivot.values.astype(float)\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 6), constrained_layout=True)\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    im = ax.imshow(data, aspect=\"auto\")\n",
    "\n",
    "    ax.set_yticks(np.arange(pivot.shape[0]))\n",
    "    ax.set_yticklabels(pivot.index)\n",
    "\n",
    "    ax.set_xticks(np.arange(pivot.shape[1]))\n",
    "    ax.set_xticklabels(group_labels, rotation=0)\n",
    "\n",
    "    ax.set_xlabel(\"Task\", labelpad=10)\n",
    "    ax.set_ylabel(\"Dentist\", labelpad=10)\n",
    "    ax.set_title(title)\n",
    "\n",
    "    fig.colorbar(im, ax=ax)\n",
    "\n",
    "    for i in range(pivot.shape[0]):\n",
    "        for j in range(pivot.shape[1]):\n",
    "            v = data[i, j]\n",
    "            txt = \"NA\" if np.isnan(v) else value_fmt.format(v)\n",
    "            ax.text(j, i, txt, ha=\"center\", va=\"center\")\n",
    "\n",
    "    fig.savefig(out_path, dpi=200)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "plots_dir = out_dir / \"plots\"\n",
    "plots_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "GROUPS = {\n",
    "    \"mesial\":    {\"tasks\": [\"mesial_binary\"],          \"type\": \"binary\"},\n",
    "    \"distal\":    {\"tasks\": [\"distal_binary\"],          \"type\": \"binary\"},\n",
    "    \"PBL\":       {\"tasks\": [\"mesial\", \"distal\"],       \"type\": \"multiclass\"},  # <-- changed\n",
    "    \"ARR\":       {\"tasks\": [\"arr_left\", \"arr_right\"],  \"type\": \"binary\"},\n",
    "    \"Furcation\": {\"tasks\": [\"furcation\"],              \"type\": \"binary\"},\n",
    "    \"PLS\":       {\"tasks\": [\"mesial_pls\", \"distal_pls\"], \"type\": \"binary\"},\n",
    "}\n",
    "\n",
    "KNOWN_TASKS = sorted({t for spec in GROUPS.values() for t in spec[\"tasks\"]}, key=len, reverse=True)\n",
    "\n",
    "\n",
    "def collect_confusion_files_flat(folder: Path, pattern: str = \"*confusion_matrix.csv\") -> List[Path]:\n",
    "    return sorted(folder.glob(pattern))\n",
    "\n",
    "\n",
    "def infer_dentist_and_task(csv_path: Path, known_tasks: List[str]) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Infer task by matching the filename (minus confusion_matrix suffix) to a known task.\n",
    "    Dentist is whatever precedes the task, or 'ALL' if none.\n",
    "    \"\"\"\n",
    "    base = infer_task_name(csv_path)\n",
    "\n",
    "    for t in known_tasks:\n",
    "        if base == t:\n",
    "            return \"ALL\", t\n",
    "        for sep in [\"__\", \"_\", \"-\"]:\n",
    "            suf = f\"{sep}{t}\"\n",
    "            if base.endswith(suf):\n",
    "                dentist_raw = base[: -len(suf)].strip(\"_-\")\n",
    "                return (dentist_raw if dentist_raw else \"ALL\"), t\n",
    "\n",
    "    raise ValueError(\n",
    "        f\"Could not infer task from filename '{csv_path.name}'. Base='{base}'. \"\n",
    "        f\"Expected it to end with one of known tasks: {known_tasks}\"\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "all_rows = []\n",
    "csv_files = collect_confusion_files_flat(root)\n",
    "\n",
    "if not csv_files:\n",
    "    raise RuntimeError(f\"No confusion_matrix.csv files found directly under: {root}\")\n",
    "\n",
    "skipped = []\n",
    "dentist_raws_seen = set()\n",
    "\n",
    "for csv_path in csv_files:\n",
    "    try:\n",
    "        dentist_raw, task = infer_dentist_and_task(csv_path, KNOWN_TASKS)\n",
    "    except ValueError:\n",
    "        skipped.append(csv_path.name)\n",
    "        continue\n",
    "\n",
    "    cm = load_confusion_csv(csv_path)\n",
    "\n",
    "    dentist_raws_seen.add(dentist_raw)\n",
    "    all_rows.append({\n",
    "        \"dentist_raw\": dentist_raw,\n",
    "        \"task\": task,\n",
    "        \"file\": str(csv_path),\n",
    "        \"cm\": cm,\n",
    "    })\n",
    "\n",
    "all_df = pd.DataFrame(all_rows)\n",
    "if all_df.empty:\n",
    "    raise RuntimeError(\n",
    "        f\"No recognised confusion matrix files found under: {root}\\n\"\n",
    "        f\"Skipped: {skipped}\"\n",
    "    )\n",
    "\n",
    "dentist_raws = sorted(dentist_raws_seen)\n",
    "dentist_map = {raw: f\"D{i+1}\" for i, raw in enumerate(dentist_raws)}\n",
    "all_df[\"dentist_id\"] = all_df[\"dentist_raw\"].map(dentist_map)\n",
    "\n",
    "if skipped:\n",
    "    print(\"Skipped files (not part of configured tasks):\")\n",
    "    for s in skipped:\n",
    "        print(f\"  - {s}\")\n",
    "\n",
    "\n",
    "\n",
    "per_group_file_rows = []\n",
    "per_group_dentist_rows = []\n",
    "\n",
    "for group_name, spec in GROUPS.items():\n",
    "    tasks = set(spec[\"tasks\"])\n",
    "    group_type = spec[\"type\"]\n",
    "\n",
    "    group_out = out_dir / group_name\n",
    "    group_out.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    gdf = all_df[all_df[\"task\"].isin(tasks)].copy()\n",
    "\n",
    "    # Per-file metrics\n",
    "    for _, r in gdf.iterrows():\n",
    "        cm = r[\"cm\"]\n",
    "\n",
    "        if group_type == \"binary\":\n",
    "            if cm.shape != (2, 2):\n",
    "                raise ValueError(f\"[{group_name}] Expected binary 2x2 but got {cm.shape} in {r['file']}\")\n",
    "            b = binary_counts_from_cm(cm, \"False\", \"True\")\n",
    "            m = metrics_from_binary_counts(b)\n",
    "        else:\n",
    "            m = metrics_multiclass(cm)\n",
    "\n",
    "        per_group_file_rows.append({\n",
    "            \"group\": group_name,\n",
    "            \"dentist_raw\": r[\"dentist_raw\"],\n",
    "            \"dentist_id\": r[\"dentist_id\"],\n",
    "            \"task\": r[\"task\"],\n",
    "            \"file\": r[\"file\"],\n",
    "            **m\n",
    "        })\n",
    "\n",
    "    # Per-dentist aggregated metrics\n",
    "    for dentist_raw, sub in gdf.groupby(\"dentist_raw\"):\n",
    "        dentist_id = dentist_map[dentist_raw]\n",
    "        cms = list(sub[\"cm\"].values)\n",
    "        agg = aggregate_confusions(cms)\n",
    "\n",
    "        if agg.empty:\n",
    "            continue\n",
    "\n",
    "        if group_type == \"binary\":\n",
    "            if agg.shape != (2, 2):\n",
    "                raise ValueError(f\"[{group_name}] Expected aggregated binary 2x2 but got {agg.shape} for {dentist_raw}\")\n",
    "            b = binary_counts_from_cm(agg, \"False\", \"True\")\n",
    "            m = metrics_from_binary_counts(b)\n",
    "            cis = bootstrap_binary_cis(b, n_boot=n_boot, seed=seed)\n",
    "\n",
    "            row = {\"group\": group_name, \"dentist_raw\": dentist_raw, \"dentist_id\": dentist_id, **m}\n",
    "            for k, (lo, hi) in cis.items():\n",
    "                row[f\"{k}_ci_low\"] = lo\n",
    "                row[f\"{k}_ci_high\"] = hi\n",
    "            per_group_dentist_rows.append(row)\n",
    "        else:\n",
    "            m = metrics_multiclass(agg)\n",
    "            per_group_dentist_rows.append({\"group\": group_name, \"dentist_raw\": dentist_raw, \"dentist_id\": dentist_id, **m})\n",
    "\n",
    "    # Save group CSVs\n",
    "    per_file_g = pd.DataFrame([r for r in per_group_file_rows if r[\"group\"] == group_name])\n",
    "    per_dent_g = pd.DataFrame([r for r in per_group_dentist_rows if r[\"group\"] == group_name])\n",
    "\n",
    "    per_file_g.to_csv(group_out / f\"per_file_metrics_{group_name}.csv\", index=False)\n",
    "    per_dent_g.to_csv(group_out / f\"per_dentist_metrics_{group_name}.csv\", index=False)\n",
    "\n",
    "# Save global combined tables\n",
    "per_file = pd.DataFrame(per_group_file_rows)\n",
    "per_dentist = pd.DataFrame(per_group_dentist_rows)\n",
    "per_file.to_csv(out_dir / \"per_file_metrics_ALLGROUPS.csv\", index=False)\n",
    "per_dentist.to_csv(out_dir / \"per_dentist_metrics_ALLGROUPS.csv\", index=False)\n",
    "\n",
    "plot_groups = [\"PBL\", \"ARR\", \"PLS\", \"Furcation\"]\n",
    "plot_labels = [\"PBL (mesial+distal, multiclass)\", \"ARR\", \"PLS\", \"Furcation\"]  # <-- updated label\n",
    "\n",
    "plot_df = per_dentist[per_dentist[\"group\"].isin(plot_groups)].copy()\n",
    "\n",
    "plot_metric_heatmap(\n",
    "    per_group_dentist=plot_df,\n",
    "    metric=\"false_negative_rate\",\n",
    "    out_path=plots_dir / \"heatmap_false_negative_rate.png\",\n",
    "    title=\"False Negative Rate (1 - recall)\",\n",
    "    group_order=plot_groups,\n",
    "    group_labels=plot_labels,\n",
    ")\n",
    "\n",
    "plot_metric_heatmap(\n",
    "    per_group_dentist=plot_df,\n",
    "    metric=\"false_positive_rate\",\n",
    "    out_path=plots_dir / \"heatmap_false_positive_rate.png\",\n",
    "    title=\"False Positive Rate (1 - specificity)\",\n",
    "    group_order=plot_groups,\n",
    "    group_labels=plot_labels,\n",
    ")\n",
    "\n",
    "print(\"Done. Outputs written to:\")\n",
    "print(f\"  {out_dir}\")\n",
    "print(\"Plots written to:\")\n",
    "print(f\"  {plots_dir}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbde5d98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
